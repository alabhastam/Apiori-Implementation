{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"<div style=\"\n    background-color: #1e1e1e; \n    color: #e0e0e0; \n    font-family: 'Consolas', 'Monaco', 'Courier New', monospace; \n    padding: 25px; \n    border-radius: 8px; \n    border: 1px solid #333; \n    box-shadow: 0 4px 15px rgba(0,0,0,0.5);\n    max-width: 950px;\">\n\n<h1 style=\"\n        color: #4fc3f7; \n        border-bottom: 2px solid #0288d1; \n        padding-bottom: 10px; \n        margin-top: 0;\n        font-family: 'Segoe UI', sans-serif;\n        letter-spacing: 1px;\">\n        &lt;Optimization: Direct Hashing & Pruning (DHP) /&gt;\n    </h1>\n\n <p style=\"font-size: 1.05em; color: #b0bec5;\">\n        The standard Apriori algorithm faces a critical bottleneck during the generation of \n        <strong style=\"color: #fff;\">2-itemsets (C‚ÇÇ)</strong>. DHP addresses this by aggressively pruning candidate pairs early using a hash-based filtering technique.\n    </p>\n\n<div style=\"\n        background-color: #263238; \n        border-left: 5px solid #ff5252; \n        padding: 15px; \n        margin: 25px 0; \n        border-radius: 4px;\">\n        <h3 style=\"margin-top: 0; color: #ff8a80; font-family: 'Segoe UI', sans-serif;\">\n            ‚ö†Ô∏è The Bottleneck: Combinatorial Explosion\n        </h3>\n        <p style=\"margin-bottom: 0; font-size: 0.95em;\">\n            If you have 1,000 frequent items in <code>L‚ÇÅ</code>, standard Apriori must generate \n            <code style=\"color: #ff5252; background-color: rgba(255,255,255,0.1); padding: 2px 6px; border-radius: 4px;\">C(1000, 2) ‚âà 500,000</code> \n            candidate pairs. Checking all these against the database is slow and memory-intensive.\n        </p>\n    </div>\n\n<h2 style=\"color: #81d4fa; font-family: 'Segoe UI', sans-serif;\">// How DHP Works</h2>\n    <p>\n        DHP <strong>\"hacks\" the first pass</strong>. While counting single items (k=1), it simultaneously gathers data about pairs.\n    </p>\n\n<div style=\"margin-left: 10px;\">\n        <div style=\"margin-bottom: 15px;\">\n            <strong style=\"color: #fff; font-size: 1.1em;\">1. The Hash Function</strong><br>\n            During the initial scan, every pair of items in a transaction is passed through a function:\n            <div style=\"\n                background-color: #000; \n                color: #a5d6a7; \n                padding: 10px; \n                border-radius: 4px; \n                margin: 10px 0; \n                border-left: 3px solid #66bb6a;\">\n                bucket_index = ( (order(x) * 10) + order(y) ) % N\n            </div>\n        </div>\n\n <div style=\"margin-bottom: 15px;\">\n            <strong style=\"color: #fff; font-size: 1.1em;\">2. The Bucket Count</strong><br>\n            A Hash Table tracks counts. If a pair maps to bucket #5, we increment bucket #5. We don't store <em>which</em> pair it was, just the frequency.\n        </div>\n\n<div>\n            <strong style=\"color: #fff; font-size: 1.1em;\">3. The Golden Rule (Pruning)</strong><br>\n            After the scan, we check the buckets. <br>\n            <span style=\"color: #ffcc80;\">If a bucket's count < min_support, ALL pairs mapping to that bucket are discarded immediately.</span>\n        </div>\n    </div>\n\n<div style=\"\n        background-color: #1b5e20; \n        background: linear-gradient(145deg, #1b5e20 0%, #2e7d32 100%);\n        padding: 20px; \n        margin: 30px 0; \n        border-radius: 6px; \n        color: #e8f5e9;\">\n        <h3 style=\"margin-top: 0; color: #fff; font-family: 'Segoe UI', sans-serif;\">üí° Concrete Example</h3>\n        <p style=\"margin: 5px 0;\"><strong>Scenario:</strong> <code>min_support = 10</code></p>\n        <ul style=\"list-style-type: square; padding-left: 20px;\">\n            <li>We analyze pair <code>{Milk, Bread}</code>.</li>\n            <li>Hash function maps it to <strong>Bucket #5</strong>.</li>\n            <li>Total count in Bucket #5 is found to be <strong>8</strong>.</li>\n        </ul>\n        <hr style=\"border: 0; border-top: 1px solid rgba(255,255,255,0.3); margin: 10px 0;\">\n        <p style=\"margin-bottom: 0; font-weight: bold;\">\n            Conclusion: Even if all 8 hits were {Milk, Bread}, 8 < 10. This pair is impossible. DROP IT.\n        </p>\n    </div>\n\n<h2 style=\"color: #81d4fa; font-family: 'Segoe UI', sans-serif;\">// Performance Comparison</h2>\n    \n <table style=\"width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.95em;\">\n        <thead>\n            <tr style=\"border-bottom: 2px solid #4fc3f7;\">\n                <th style=\"text-align: left; padding: 12px; color: #4fc3f7;\">Feature</th>\n                <th style=\"text-align: left; padding: 12px; color: #b0bec5;\">Standard Apriori</th>\n                <th style=\"text-align: left; padding: 12px; color: #fff;\">Apriori + DHP</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr style=\"border-bottom: 1px solid #424242;\">\n                <td style=\"padding: 12px; color: #e0e0e0;\"><strong>Candidate Gen (C‚ÇÇ)</strong></td>\n                <td style=\"padding: 12px; color: #9e9e9e;\">Blindly joins all L‚ÇÅ items ($\\approx n^2/2$).</td>\n                <td style=\"padding: 12px; color: #81c784;\">Only generates pairs from frequent buckets.</td>\n            </tr>\n            <tr style=\"border-bottom: 1px solid #424242;\">\n                <td style=\"padding: 12px; color: #e0e0e0;\"><strong>DB Size</strong></td>\n                <td style=\"padding: 12px; color: #9e9e9e;\">Full scan every time.</td>\n                <td style=\"padding: 12px; color: #81c784;\">Can \"trim\" DB by removing useless transactions.</td>\n            </tr>\n        </tbody>\n    </table>\n\n</div>\n","metadata":{}},{"cell_type":"code","source":"import pandas as pd\nfrom collections import defaultdict\nfrom itertools import combinations\nimport tracemalloc\nimport time\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:59:53.320438Z","iopub.execute_input":"2025-11-29T09:59:53.321023Z","iopub.status.idle":"2025-11-29T09:59:53.333408Z","shell.execute_reply.started":"2025-11-29T09:59:53.320990Z","shell.execute_reply":"2025-11-29T09:59:53.331403Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"\n# The Raw Data\nraw_data = [\n    ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n    ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n    ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n    ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n    ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'] # Note duplicates\n]\n\n# Convert to list of sets (removes duplicate 'Onion' in last row)\ndataset = [set(transaction) for transaction in raw_data]\n\n# 2. Parameters\nMIN_SUPPORT = 2\nBUCKET_COUNT = 7  \n\n# Create a Mapping (String -> Integer) for the Hash Function\n# We sort all unique items alphabetically to ensure deterministic IDs\nunique_items = sorted(set(item for sublist in dataset for item in sublist))\nitem_to_id = {item: i+1 for i, item in enumerate(unique_items)}\n\n# Define the Hash Function\ndef get_hash_bucket(item1, item2, bucket_count=BUCKET_COUNT):\n    \n    # Get IDs\n    id1 = item_to_id[item1]\n    id2 = item_to_id[item2]\n    \n    # Ensure order to make hash commutative: hash(A, B) == hash(B, A)\n    first = min(id1, id2)\n    second = max(id1, id2)\n    \n    # Calculate Hash\n    val = (first * 10) + second\n    return val % bucket_count\n\n# --- DISPLAY SETUP ---\nprint(f\"--- CONFIGURATION ---\")\nprint(f\"Transaction Count: {len(dataset)}\")\nprint(f\"Min Support: {MIN_SUPPORT}\")\nprint(f\"Hash Buckets: {BUCKET_COUNT}\")\nprint(\"-\" * 30)\nprint(\"Item ID Mapping (for Hashing):\")\nfor item, idx in item_to_id.items():\n    print(f\"  {item}: {idx}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:28:37.558194Z","iopub.execute_input":"2025-11-29T09:28:37.558524Z","iopub.status.idle":"2025-11-29T09:28:39.913243Z","shell.execute_reply.started":"2025-11-29T09:28:37.558500Z","shell.execute_reply":"2025-11-29T09:28:39.912248Z"}},"outputs":[{"name":"stdout","text":"--- CONFIGURATION ---\nTransaction Count: 5\nMin Support: 2\nHash Buckets: 7\n------------------------------\nItem ID Mapping (for Hashing):\n  Apple: 1\n  Corn: 2\n  Dill: 3\n  Eggs: 4\n  Ice cream: 5\n  Kidney Beans: 6\n  Milk: 7\n  Nutmeg: 8\n  Onion: 9\n  Unicorn: 10\n  Yogurt: 11\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"\n\n# Initialization\nC1_counts = defaultdict(int)\nbucket_counts = defaultdict(int) # This acts as our Hash Table\n\nprint(\"STARTING SCAN 1\")\n\nfor t_idx, transaction in enumerate(dataset):\n    # though sets are fine for combinations\n    items = list(transaction)\n    \n    for item in items:\n        C1_counts[item] += 1\n        \n    #  DHP Special: Hash all 2-item subsets\n    # Generate all pairs (combinations of size 2)\n    pairs = list(combinations(items, 2))\n    \n    for pair in pairs:\n        bucket_idx = get_hash_bucket(pair[0], pair[1])\n        bucket_counts[bucket_idx] += 1\n        \n    # Optional: Print detail for first transaction only to avoid clutter\n    if t_idx == 0:\n        print(f\"Transaction 1 Pairs processed: {len(pairs)}\")\n        print(f\"Example Pair from T1: {pairs[0]} -> Bucket {get_hash_bucket(pairs[0][0], pairs[0][1])}\")\n\nprint(\"\\n--- SCAN COMPLETE ---\")\n\n# --- FILTERING L1 (Standard Part) ---\n# Filter items that meet MIN_SUPPORT\nL1 = {item: count for item, count in C1_counts.items() if count >= MIN_SUPPORT}\nsorted_L1 = sorted(L1.items(), key=lambda x: x[1], reverse=True)\n\nprint(f\"\\n[L1 Result] Frequent 1-Itemsets (Count >= {MIN_SUPPORT}):\")\nprint(sorted_L1)\n\n# DHP BUCKET RESULTS: nothing pruned here. \nprint(f\"\\n[DHP Result] Hash Table Status (Bucket Counts):\")\nprint(f\"{'Bucket ID':<10} | {'Count':<10} | {'Status (Keep/Prune?)'}\")\nprint(\"-\" * 45)\n\nvalid_buckets = [] # To store buckets that passed the test\nfor i in range(BUCKET_COUNT):\n    count = bucket_counts[i]\n    is_valid = count >= MIN_SUPPORT\n    status = \" KEEP\" if is_valid else \"PRUNE\"\n    if is_valid:\n        valid_buckets.append(i)\n        \n    print(f\"{i:<10} | {count:<10} | {status}\")\n\nprint(f\"\\nValid Buckets for Next Step: {valid_buckets}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:41:01.273251Z","iopub.execute_input":"2025-11-29T09:41:01.273615Z","iopub.status.idle":"2025-11-29T09:41:01.285735Z","shell.execute_reply.started":"2025-11-29T09:41:01.273590Z","shell.execute_reply":"2025-11-29T09:41:01.284361Z"}},"outputs":[{"name":"stdout","text":"STARTING SCAN 1\nTransaction 1 Pairs processed: 15\nExample Pair from T1: ('Onion', 'Eggs') -> Bucket 0\n\n--- SCAN COMPLETE ---\n\n[L1 Result] Frequent 1-Itemsets (Count >= 2):\n[('Kidney Beans', 5), ('Eggs', 4), ('Onion', 3), ('Yogurt', 3), ('Milk', 3), ('Nutmeg', 2), ('Corn', 2)]\n\n[DHP Result] Hash Table Status (Bucket Counts):\nBucket ID  | Count      | Status (Keep/Prune?)\n---------------------------------------------\n0          | 8          |  KEEP\n1          | 6          |  KEEP\n2          | 5          |  KEEP\n3          | 9          |  KEEP\n4          | 11         |  KEEP\n5          | 8          |  KEEP\n6          | 9          |  KEEP\n\nValid Buckets for Next Step: [0, 1, 2, 3, 4, 5, 6]\n","output_type":"stream"}],"execution_count":3},{"cell_type":"markdown","source":"Every bucket get keeped because of: B= 7. It's critical to save this.","metadata":{}},{"cell_type":"code","source":"# L1 keys (items)\nl1_items = [x[0] for x in sorted_L1] \n\nprint(f\"--- GENERATING C2 CANDIDATES ---\")\nprint(f\"Items in L1: {len(l1_items)}\")\nprint(f\"Valid Buckets: {valid_buckets}\")\n\nC2_candidates = []\nrejected_count = 0\n\n# Generate all pairs from L1 items\npossible_pairs = list(combinations(l1_items, 2))\n\nprint(f\"Total possible pairs from L1: {len(possible_pairs)}\")\nprint(\"-\" * 40)\n\nfor pair in possible_pairs:\n    item1, item2 = pair\n    \n    # 1. Calculate Hash for this candidate\n    bucket = get_hash_bucket(item1, item2)\n    \n    # 2. DHP CHECK: Is this bucket in our valid list?\n    if bucket in valid_buckets:\n        C2_candidates.append(frozenset(pair))\n        # purely for display:\n        # print(f\"  Accepted: {pair} (Bucket {bucket})\") \n    else:\n        rejected_count += 1\n        print(f\"PRUNED by DHP: {pair} (Bucket {bucket})\")\n\nprint(\"-\" * 40)\nprint(f\"Candidates Generated (C2 Size): {len(C2_candidates)}\")\nprint(f\"Candidates Pruned by DHP: {rejected_count}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:55:06.942538Z","iopub.execute_input":"2025-11-29T09:55:06.942861Z","iopub.status.idle":"2025-11-29T09:55:06.951137Z","shell.execute_reply.started":"2025-11-29T09:55:06.942838Z","shell.execute_reply":"2025-11-29T09:55:06.950329Z"}},"outputs":[{"name":"stdout","text":"--- GENERATING C2 CANDIDATES ---\nItems in L1: 7\nValid Buckets: [0, 1, 2, 3, 4, 5, 6]\nTotal possible pairs from L1: 21\n----------------------------------------\n----------------------------------------\nCandidates Generated (C2 Size): 21\nCandidates Pruned by DHP: 0\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"def load_dataset():\n    return [\n        frozenset(['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt']),\n        frozenset(['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt']),\n        frozenset(['Milk', 'Apple', 'Kidney Beans', 'Eggs']),\n        frozenset(['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt']),\n        frozenset(['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'])\n    ]\n\ndef get_item_id(item_name):\n    \"\"\"Mapping items to integers for hashing.\"\"\"\n    mapping = {\n        'Apple': 1, 'Corn': 2, 'Dill': 3, 'Eggs': 4, \n        'Ice cream': 5, 'Kidney Beans': 6, 'Milk': 7, \n        'Nutmeg': 8, 'Onion': 9, 'Unicorn': 10, 'Yogurt': 11\n    }\n    return mapping.get(item_name, 0)\n\ndef get_hash_bucket(item1, item2, bucket_count=7):\n    id1 = get_item_id(item1)\n    id2 = get_item_id(item2)\n    # Ensuring order doesn't change hash (commutative)\n    if id1 > id2: id1, id2 = id2, id1\n    return (id1 * 10 + id2) % bucket_count\n\n# --- 2. Core Functions ---\n\ndef apriori_gen(Lk_minus_1, k):\n    \"\"\"\n    Generates Ck from L(k-1).\n    For k=2, we use simple combinations.\n    For k>2, we join sets that share k-2 items.\n    \"\"\"\n    candidates = []\n    len_Lk = len(Lk_minus_1)\n    lk_list = list(Lk_minus_1)\n    \n    if k == 2:\n        # Simple pairs\n        return list(combinations(lk_list, 2))\n    \n    # Standard Apriori Join for k > 2\n    for i in range(len_Lk):\n        for j in range(i + 1, len_Lk):\n            L1 = list(lk_list[i])\n            L2 = list(lk_list[j])\n            L1.sort(); L2.sort()\n            \n            # If first k-2 elements are equal, join them\n            if L1[:k-2] == L2[:k-2]:\n                candidates.append(lk_list[i] | lk_list[j])\n                \n    return candidates\n\ndef scan_and_count(dataset, candidates):\n    \"\"\"Counts support for a list of candidates.\"\"\"\n    counts = defaultdict(int)\n    for tid in dataset:\n        for cand in candidates:\n            # cand can be a tuple (from combinations) or frozenset\n            cand_set = frozenset(cand)\n            if cand_set.issubset(tid):\n                counts[cand_set] += 1\n    return counts\n\n# --- 3. Main Algorithm with Metrics ---\n\ndef run_apriori_dhp(min_support=2):\n    # Start Metrics\n    tracemalloc.start()\n    start_time = time.time()\n    \n    dataset = load_dataset()\n    bucket_count = 7\n    \n    global_frequent_itemsets = {} # Final Result Storage\n    \n    print(\"--- Step 1: Init Pass (Count L1 + Hash C2) ---\")\n    c1_counts = defaultdict(int)\n    bucket_counts = defaultdict(int)\n    \n    for transaction in dataset:\n        # Count Items\n        for item in transaction:\n            c1_counts[item] += 1\n            \n        # Hash Pairs (DHP)\n        items = list(transaction)\n        for i in range(len(items)):\n            for j in range(i + 1, len(items)):\n                b_idx = get_hash_bucket(items[i], items[j], bucket_count)\n                bucket_counts[b_idx] += 1\n\n    # Generate L1\n    L1 = [item for item, count in c1_counts.items() if count >= min_support]\n    L1.sort() # Sorting helps deterministic behavior\n    \n    # Store L1\n    for item in L1:\n        global_frequent_itemsets[frozenset([item])] = c1_counts[item]\n\n    print(f\"L1 found: {len(L1)} items\")\n    \n    # --- Step 2: Generate C2 using DHP Filter ---\n    print(\"--- Step 2: Generate C2 (with DHP Pruning) ---\")\n    valid_buckets = {b for b, cnt in bucket_counts.items() if cnt >= min_support}\n    \n    C2_candidates = []\n    l1_pairs = list(combinations(L1, 2))\n    \n    for pair in l1_pairs:\n        b_idx = get_hash_bucket(pair[0], pair[1], bucket_count)\n        if b_idx in valid_buckets:\n            C2_candidates.append(frozenset(pair))\n    \n    print(f\"C2 Candidates after DHP: {len(C2_candidates)} (out of {len(l1_pairs)} possible)\")\n    \n    # Loop Variables\n    current_C = C2_candidates\n    k = 2\n    \n    # --- Step 3: Iterative Loop (L2, L3...) ---\n    while len(current_C) > 0:\n        print(f\"--- Scanning DB for k={k} ---\")\n        \n        # Count Support\n        candidates_counts = scan_and_count(dataset, current_C)\n        \n        # Filter L_k\n        L_k = []\n        for cand, count in candidates_counts.items():\n            if count >= min_support:\n                L_k.append(cand)\n                global_frequent_itemsets[cand] = count\n        \n        print(f\"L{k} found: {len(L_k)} itemsets\")\n        \n        if len(L_k) <= 1:\n            break # Cannot generate C_(k+1) from 1 or 0 items\n            \n        # Generate Next Candidates (C_k+1) - Standard Join\n        k += 1\n        current_C = apriori_gen(L_k, k)\n        \n    # End Metrics\n    end_time = time.time()\n    current_mem, peak_mem = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    \n    execution_time = (end_time - start_time) * 1000 # ms\n    peak_mem_mb = peak_mem / (1024 * 1024)\n    \n    return global_frequent_itemsets, execution_time, peak_mem_mb\n\n# --- 4. Execution ---\n\nfinal_results, exec_time, peak_memory = run_apriori_dhp(min_support=2)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"FINAL OUTPUT\")\nprint(\"=\"*50)\nprint(f\"Total Frequent Itemsets Found: {len(final_results)}\")\nprint(f\"Execution Time: {exec_time:.4f} ms\")\nprint(f\"Peak Memory Usage: {peak_memory:.6f} MB\")\nprint(\"-\" * 50)\nprint(\"{Itemset: Support}\")\nfor itemset, support in final_results.items():\n    # formatting frozenset to look cleaner\n    clean_set = set(itemset) \n    print(f\"{clean_set}: {support}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T09:59:58.093016Z","iopub.execute_input":"2025-11-29T09:59:58.093625Z","iopub.status.idle":"2025-11-29T09:59:58.226748Z","shell.execute_reply.started":"2025-11-29T09:59:58.093589Z","shell.execute_reply":"2025-11-29T09:59:58.225478Z"}},"outputs":[{"name":"stdout","text":"--- Step 1: Init Pass (Count L1 + Hash C2) ---\nL1 found: 7 items\n--- Step 2: Generate C2 (with DHP Pruning) ---\nC2 Candidates after DHP: 21 (out of 21 possible)\n--- Scanning DB for k=2 ---\nL2 found: 14 itemsets\n--- Scanning DB for k=3 ---\nL3 found: 12 itemsets\n--- Scanning DB for k=4 ---\nL4 found: 5 itemsets\n--- Scanning DB for k=5 ---\nL5 found: 1 itemsets\n\n==================================================\nFINAL OUTPUT\n==================================================\nTotal Frequent Itemsets Found: 39\nExecution Time: 3.6924 ms\nPeak Memory Usage: 0.586279 MB\n--------------------------------------------------\n{Itemset: Support}\n{'Corn'}: 2\n{'Eggs'}: 4\n{'Kidney Beans'}: 5\n{'Milk'}: 3\n{'Nutmeg'}: 2\n{'Onion'}: 3\n{'Yogurt'}: 3\n{'Kidney Beans', 'Eggs'}: 4\n{'Eggs', 'Milk'}: 2\n{'Eggs', 'Nutmeg'}: 2\n{'Onion', 'Eggs'}: 3\n{'Eggs', 'Yogurt'}: 2\n{'Kidney Beans', 'Milk'}: 3\n{'Kidney Beans', 'Nutmeg'}: 2\n{'Kidney Beans', 'Onion'}: 3\n{'Kidney Beans', 'Yogurt'}: 3\n{'Yogurt', 'Milk'}: 2\n{'Onion', 'Nutmeg'}: 2\n{'Yogurt', 'Nutmeg'}: 2\n{'Onion', 'Yogurt'}: 2\n{'Kidney Beans', 'Corn'}: 2\n{'Kidney Beans', 'Eggs', 'Milk'}: 2\n{'Kidney Beans', 'Eggs', 'Nutmeg'}: 2\n{'Kidney Beans', 'Onion', 'Eggs'}: 3\n{'Kidney Beans', 'Eggs', 'Yogurt'}: 2\n{'Onion', 'Eggs', 'Nutmeg'}: 2\n{'Eggs', 'Yogurt', 'Nutmeg'}: 2\n{'Onion', 'Eggs', 'Yogurt'}: 2\n{'Kidney Beans', 'Yogurt', 'Milk'}: 2\n{'Kidney Beans', 'Onion', 'Nutmeg'}: 2\n{'Kidney Beans', 'Yogurt', 'Nutmeg'}: 2\n{'Kidney Beans', 'Onion', 'Yogurt'}: 2\n{'Onion', 'Yogurt', 'Nutmeg'}: 2\n{'Kidney Beans', 'Onion', 'Nutmeg', 'Eggs'}: 2\n{'Kidney Beans', 'Eggs', 'Yogurt', 'Nutmeg'}: 2\n{'Kidney Beans', 'Onion', 'Eggs', 'Yogurt'}: 2\n{'Onion', 'Eggs', 'Yogurt', 'Nutmeg'}: 2\n{'Kidney Beans', 'Onion', 'Yogurt', 'Nutmeg'}: 2\n{'Kidney Beans', 'Nutmeg', 'Eggs', 'Onion', 'Yogurt'}: 2\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"<div style=\"font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; color: #e0e0e0; background-color: #121212; padding: 20px; border-radius: 8px;\">\n\n <div style=\"background-color: #1f1f1f; border-left: 5px solid #bb86fc; padding: 15px; margin-bottom: 20px; border-radius: 4px; box-shadow: 0 4px 6px rgba(0,0,0,0.3);\">\n        <h1 style=\"margin:0; color: #ffffff; font-weight: 300; letter-spacing: 1px;\">Sampling Technique in Association Rule Mining</h1>\n        <p style=\"margin-top: 5px; font-size: 1.1em; color: #b0b0b0;\">A Data-Driven Optimization Strategy</p>\n    </div>\n\n<p style=\"color: #cccccc;\">\n        While algorithms like <strong style=\"color: #bb86fc;\">DHP (Direct Hashing and Pruning)</strong> optimize <i>how</i> we process data, <strong style=\"color: #03dac6;\">Sampling</strong> optimizes <i>what</i> data we process. \n        In massive datasets (e.g., millions of transactions), running a full Apriori scan is computationally expensive. Sampling addresses this by analyzing a random subset of data and generalizing the results.\n    </p>\n\n<div style=\"background-color: #2c2c2c; border: 1px solid #cf6679; border-radius: 5px; padding: 15px; margin: 25px 0;\">\n        <h3 style=\"margin-top: 0; color: #ff8a80;\">‚ö° The Core Challenge: Threshold Adjustment</h3>\n        <p>\n            When you reduce the dataset size, you cannot simply use the same absolute support count. More importantly, you face the <b style=\"color: #ffcc80;\">Variance Problem</b>: A frequent item might accidentally appear less frequently in your random sample than in the full DB.\n        </p>\n        <p><b style=\"color: #81c784;\">The Solution: Lowered Support Threshold</b></p>\n        <p>We apply a \"Safety Margin\" ($1 - \\epsilon$) to the support threshold to avoid False Negatives.</p>\n        \n<div style=\"background-color: #121212; padding: 15px; border-radius: 4px; text-align: center; font-family: 'Courier New', monospace; font-weight: bold; border: 1px solid #444; color: #80cbc4;\">\n            $$ S_{sample} = (S_{db\\_fraction} \\times N_{sample}) \\times (1 - \\epsilon) $$\n        </div>\n        <ul style=\"font-size: 0.9em; color: #b0b0b0; margin-top: 10px;\">\n            <li><span style=\"color:#80cbc4\">$S_{sample}$</span>: New Minimum Support Count for the sample.</li>\n            <li><span style=\"color:#80cbc4\">$S_{db\\_fraction}$</span>: The target support percentage (e.g., 0.01 for 1%).</li>\n            <li><span style=\"color:#80cbc4\">$\\epsilon$</span>: The relaxation factor (e.g., 0.1 for a 10% safety buffer).</li>\n        </ul>\n    </div>\n\n<h3 style=\"color: #03dac6; border-bottom: 1px solid #333; padding-bottom: 5px;\">Step-by-Step Algorithm</h3>\n    <ol style=\"color: #cccccc;\">\n        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Select Sample:</b> Pick a random subset of size $M$ from the original database $N$.</li>\n        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Adjust Threshold:</b> Calculate the new support using the safety margin formula.</li>\n        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Run Apriori:</b> Execute the standard (or DHP) Apriori on the sample $M$.</li>\n        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Identify Candidates:</b> The output is considered the \"Global Candidate Set\".</li>\n        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Verify (Optional):</b> Run one final scan on the full $N$ to verify the actual counts of these candidates (eliminating False Positives).</li>\n    </ol>\n\n\n\n <h3 style=\"color: #03dac6; border-bottom: 1px solid #333; padding-bottom: 5px; margin-top: 30px;\">Pros & Cons Analysis</h3>\n    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; border: 1px solid #444;\">\n        <thead>\n            <tr style=\"background-color: #333;\">\n                <th style=\"padding: 12px; border: 1px solid #444; text-align: left; width: 20%; color: #ffffff;\">Feature</th>\n                <th style=\"padding: 12px; border: 1px solid #444; text-align: left; color: #81c784;\">Pros (Advantages)</th>\n                <th style=\"padding: 12px; border: 1px solid #444; text-align: left; color: #e57373;\">Cons (Risks)</th>\n            </tr>\n        </thead>\n        <tbody>\n            <tr style=\"background-color: #1a1a1a;\">\n                <td style=\"padding: 12px; border: 1px solid #444; font-weight: bold; color: #eee;\">Speed</td>\n                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">üöÄ <b>Extremely Fast</b>. Reducing data by 90% can speed up execution by 100x due to combinatorial explosion.</td>\n                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">Overhead of sampling if data is not easily accessible (e.g., on disk).</td>\n            </tr>\n            <tr style=\"background-color: #222;\">\n                <td style=\"padding: 12px; border: 1px solid #444; font-weight: bold; color: #eee;\">Memory</td>\n                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">‚úÖ <b>High Efficiency</b>. Allows processing terabyte-scale DBs on standard RAM.</td>\n                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">None.</td>\n            </tr>\n            <tr style=\"background-color: #1a1a1a;\">\n                <td style=\"padding: 12px; border: 1px solid #444; font-weight: bold; color: #eee;\">Accuracy</td>\n                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">Good enough for finding \"Strong\" rules.</td>\n                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">‚ö†Ô∏è <b>False Negatives</b>. May miss rare but significant patterns if the sample is unrepresentative.</td>\n            </tr>\n        </tbody>\n    </table>\n\n <div style=\"margin-top: 30px; padding: 15px; border-top: 1px solid #333; font-size: 0.9em; color: #888; font-style: italic;\">\n        Note: This sampling logic is the foundational concept behind the <b style=\"color: #ccc;\">SON Algorithm</b> (Savasere, Omiecinski, and Navathe), which utilizes MapReduce for large-scale mining.\n    </div>\n\n</div>\n","metadata":{}}]}