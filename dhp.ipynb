{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8e927ab",
   "metadata": {
    "papermill": {
     "duration": 0.003751,
     "end_time": "2025-11-29T12:09:04.379065",
     "exception": false,
     "start_time": "2025-11-29T12:09:04.375314",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"\n",
    "    background-color: #1e1e1e; \n",
    "    color: #e0e0e0; \n",
    "    font-family: 'Consolas', 'Monaco', 'Courier New', monospace; \n",
    "    padding: 25px; \n",
    "    border-radius: 8px; \n",
    "    border: 1px solid #333; \n",
    "    box-shadow: 0 4px 15px rgba(0,0,0,0.5);\n",
    "    max-width: 950px;\">\n",
    "\n",
    "<h1 style=\"\n",
    "        color: #4fc3f7; \n",
    "        border-bottom: 2px solid #0288d1; \n",
    "        padding-bottom: 10px; \n",
    "        margin-top: 0;\n",
    "        font-family: 'Segoe UI', sans-serif;\n",
    "        letter-spacing: 1px;\">\n",
    "        &lt;Optimization: Direct Hashing & Pruning (DHP) /&gt;\n",
    "    </h1>\n",
    "\n",
    " <p style=\"font-size: 1.05em; color: #b0bec5;\">\n",
    "        The standard Apriori algorithm faces a critical bottleneck during the generation of \n",
    "        <strong style=\"color: #fff;\">2-itemsets (C‚ÇÇ)</strong>. DHP addresses this by aggressively pruning candidate pairs early using a hash-based filtering technique.\n",
    "    </p>\n",
    "\n",
    "<div style=\"\n",
    "        background-color: #263238; \n",
    "        border-left: 5px solid #ff5252; \n",
    "        padding: 15px; \n",
    "        margin: 25px 0; \n",
    "        border-radius: 4px;\">\n",
    "        <h3 style=\"margin-top: 0; color: #ff8a80; font-family: 'Segoe UI', sans-serif;\">\n",
    "            ‚ö†Ô∏è The Bottleneck: Combinatorial Explosion\n",
    "        </h3>\n",
    "        <p style=\"margin-bottom: 0; font-size: 0.95em;\">\n",
    "            If you have 1,000 frequent items in <code>L‚ÇÅ</code>, standard Apriori must generate \n",
    "            <code style=\"color: #ff5252; background-color: rgba(255,255,255,0.1); padding: 2px 6px; border-radius: 4px;\">C(1000, 2) ‚âà 500,000</code> \n",
    "            candidate pairs. Checking all these against the database is slow and memory-intensive.\n",
    "        </p>\n",
    "    </div>\n",
    "\n",
    "<h2 style=\"color: #81d4fa; font-family: 'Segoe UI', sans-serif;\">// How DHP Works</h2>\n",
    "    <p>\n",
    "        DHP <strong>\"hacks\" the first pass</strong>. While counting single items (k=1), it simultaneously gathers data about pairs.\n",
    "    </p>\n",
    "\n",
    "<div style=\"margin-left: 10px;\">\n",
    "        <div style=\"margin-bottom: 15px;\">\n",
    "            <strong style=\"color: #fff; font-size: 1.1em;\">1. The Hash Function</strong><br>\n",
    "            During the initial scan, every pair of items in a transaction is passed through a function:\n",
    "            <div style=\"\n",
    "                background-color: #000; \n",
    "                color: #a5d6a7; \n",
    "                padding: 10px; \n",
    "                border-radius: 4px; \n",
    "                margin: 10px 0; \n",
    "                border-left: 3px solid #66bb6a;\">\n",
    "                bucket_index = ( (order(x) * 10) + order(y) ) % N\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    " <div style=\"margin-bottom: 15px;\">\n",
    "            <strong style=\"color: #fff; font-size: 1.1em;\">2. The Bucket Count</strong><br>\n",
    "            A Hash Table tracks counts. If a pair maps to bucket #5, we increment bucket #5. We don't store <em>which</em> pair it was, just the frequency.\n",
    "        </div>\n",
    "\n",
    "<div>\n",
    "            <strong style=\"color: #fff; font-size: 1.1em;\">3. The Golden Rule (Pruning)</strong><br>\n",
    "            After the scan, we check the buckets. <br>\n",
    "            <span style=\"color: #ffcc80;\">If a bucket's count < min_support, ALL pairs mapping to that bucket are discarded immediately.</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "<div style=\"\n",
    "        background-color: #1b5e20; \n",
    "        background: linear-gradient(145deg, #1b5e20 0%, #2e7d32 100%);\n",
    "        padding: 20px; \n",
    "        margin: 30px 0; \n",
    "        border-radius: 6px; \n",
    "        color: #e8f5e9;\">\n",
    "        <h3 style=\"margin-top: 0; color: #fff; font-family: 'Segoe UI', sans-serif;\">üí° Concrete Example</h3>\n",
    "        <p style=\"margin: 5px 0;\"><strong>Scenario:</strong> <code>min_support = 10</code></p>\n",
    "        <ul style=\"list-style-type: square; padding-left: 20px;\">\n",
    "            <li>We analyze pair <code>{Milk, Bread}</code>.</li>\n",
    "            <li>Hash function maps it to <strong>Bucket #5</strong>.</li>\n",
    "            <li>Total count in Bucket #5 is found to be <strong>8</strong>.</li>\n",
    "        </ul>\n",
    "        <hr style=\"border: 0; border-top: 1px solid rgba(255,255,255,0.3); margin: 10px 0;\">\n",
    "        <p style=\"margin-bottom: 0; font-weight: bold;\">\n",
    "            Conclusion: Even if all 8 hits were {Milk, Bread}, 8 < 10. This pair is impossible. DROP IT.\n",
    "        </p>\n",
    "    </div>\n",
    "\n",
    "<h2 style=\"color: #81d4fa; font-family: 'Segoe UI', sans-serif;\">// Performance Comparison</h2>\n",
    "    \n",
    " <table style=\"width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.95em;\">\n",
    "        <thead>\n",
    "            <tr style=\"border-bottom: 2px solid #4fc3f7;\">\n",
    "                <th style=\"text-align: left; padding: 12px; color: #4fc3f7;\">Feature</th>\n",
    "                <th style=\"text-align: left; padding: 12px; color: #b0bec5;\">Standard Apriori</th>\n",
    "                <th style=\"text-align: left; padding: 12px; color: #fff;\">Apriori + DHP</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr style=\"border-bottom: 1px solid #424242;\">\n",
    "                <td style=\"padding: 12px; color: #e0e0e0;\"><strong>Candidate Gen (C‚ÇÇ)</strong></td>\n",
    "                <td style=\"padding: 12px; color: #9e9e9e;\">Blindly joins all L‚ÇÅ items ($\\approx n^2/2$).</td>\n",
    "                <td style=\"padding: 12px; color: #81c784;\">Only generates pairs from frequent buckets.</td>\n",
    "            </tr>\n",
    "            <tr style=\"border-bottom: 1px solid #424242;\">\n",
    "                <td style=\"padding: 12px; color: #e0e0e0;\"><strong>DB Size</strong></td>\n",
    "                <td style=\"padding: 12px; color: #9e9e9e;\">Full scan every time.</td>\n",
    "                <td style=\"padding: 12px; color: #81c784;\">Can \"trim\" DB by removing useless transactions.</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0364572e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:09:04.386527Z",
     "iopub.status.busy": "2025-11-29T12:09:04.386214Z",
     "iopub.status.idle": "2025-11-29T12:09:06.253403Z",
     "shell.execute_reply": "2025-11-29T12:09:06.252561Z"
    },
    "papermill": {
     "duration": 1.872702,
     "end_time": "2025-11-29T12:09:06.255060",
     "exception": false,
     "start_time": "2025-11-29T12:09:04.382358",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import itertools\n",
    "import tracemalloc\n",
    "import time\n",
    "import random\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96dcc756",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:09:06.262682Z",
     "iopub.status.busy": "2025-11-29T12:09:06.262273Z",
     "iopub.status.idle": "2025-11-29T12:09:06.271594Z",
     "shell.execute_reply": "2025-11-29T12:09:06.270410Z"
    },
    "papermill": {
     "duration": 0.015172,
     "end_time": "2025-11-29T12:09:06.273314",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.258142",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION ---\n",
      "Transaction Count: 5\n",
      "Min Support: 2\n",
      "Hash Buckets: 7\n",
      "------------------------------\n",
      "Item ID Mapping (for Hashing):\n",
      "  Apple: 1\n",
      "  Corn: 2\n",
      "  Dill: 3\n",
      "  Eggs: 4\n",
      "  Ice cream: 5\n",
      "  Kidney Beans: 6\n",
      "  Milk: 7\n",
      "  Nutmeg: 8\n",
      "  Onion: 9\n",
      "  Unicorn: 10\n",
      "  Yogurt: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The Raw Data\n",
    "raw_data = [\n",
    "    ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "    ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "    ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'] # Note duplicates\n",
    "]\n",
    "\n",
    "# Convert to list of sets (removes duplicate 'Onion' in last row)\n",
    "dataset = [set(transaction) for transaction in raw_data]\n",
    "\n",
    "# 2. Parameters\n",
    "MIN_SUPPORT = 2\n",
    "BUCKET_COUNT = 7  \n",
    "\n",
    "# Create a Mapping (String -> Integer) for the Hash Function\n",
    "# We sort all unique items alphabetically to ensure deterministic IDs\n",
    "unique_items = sorted(set(item for sublist in dataset for item in sublist))\n",
    "item_to_id = {item: i+1 for i, item in enumerate(unique_items)}\n",
    "\n",
    "# Define the Hash Function\n",
    "def get_hash_bucket(item1, item2, bucket_count=BUCKET_COUNT):\n",
    "    \n",
    "    # Get IDs\n",
    "    id1 = item_to_id[item1]\n",
    "    id2 = item_to_id[item2]\n",
    "    \n",
    "    # Ensure order to make hash commutative: hash(A, B) == hash(B, A)\n",
    "    first = min(id1, id2)\n",
    "    second = max(id1, id2)\n",
    "    \n",
    "    # Calculate Hash\n",
    "    val = (first * 10) + second\n",
    "    return val % bucket_count\n",
    "\n",
    "# --- DISPLAY SETUP ---\n",
    "print(f\"--- CONFIGURATION ---\")\n",
    "print(f\"Transaction Count: {len(dataset)}\")\n",
    "print(f\"Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"Hash Buckets: {BUCKET_COUNT}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Item ID Mapping (for Hashing):\")\n",
    "for item, idx in item_to_id.items():\n",
    "    print(f\"  {item}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f8f1fe7a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:09:06.280681Z",
     "iopub.status.busy": "2025-11-29T12:09:06.280390Z",
     "iopub.status.idle": "2025-11-29T12:09:06.291104Z",
     "shell.execute_reply": "2025-11-29T12:09:06.289995Z"
    },
    "papermill": {
     "duration": 0.016,
     "end_time": "2025-11-29T12:09:06.292397",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.276397",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING SCAN 1\n",
      "Transaction 1 Pairs processed: 15\n",
      "Example Pair from T1: ('Onion', 'Eggs') -> Bucket 0\n",
      "\n",
      "--- SCAN COMPLETE ---\n",
      "\n",
      "[L1 Result] Frequent 1-Itemsets (Count >= 2):\n",
      "[('Kidney Beans', 5), ('Eggs', 4), ('Onion', 3), ('Milk', 3), ('Yogurt', 3), ('Nutmeg', 2), ('Corn', 2)]\n",
      "\n",
      "[DHP Result] Hash Table Status (Bucket Counts):\n",
      "Bucket ID  | Count      | Status (Keep/Prune?)\n",
      "---------------------------------------------\n",
      "0          | 8          |  KEEP\n",
      "1          | 6          |  KEEP\n",
      "2          | 5          |  KEEP\n",
      "3          | 9          |  KEEP\n",
      "4          | 11         |  KEEP\n",
      "5          | 8          |  KEEP\n",
      "6          | 9          |  KEEP\n",
      "\n",
      "Valid Buckets for Next Step: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialization\n",
    "C1_counts = defaultdict(int)\n",
    "bucket_counts = defaultdict(int) # This acts as our Hash Table\n",
    "\n",
    "print(\"STARTING SCAN 1\")\n",
    "\n",
    "for t_idx, transaction in enumerate(dataset):\n",
    "    # though sets are fine for combinations\n",
    "    items = list(transaction)\n",
    "    \n",
    "    for item in items:\n",
    "        C1_counts[item] += 1\n",
    "        \n",
    "    #  DHP Special: Hash all 2-item subsets\n",
    "    # Generate all pairs (combinations of size 2)\n",
    "    pairs = list(combinations(items, 2))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        bucket_idx = get_hash_bucket(pair[0], pair[1])\n",
    "        bucket_counts[bucket_idx] += 1\n",
    "        \n",
    "    # Optional: Print detail for first transaction only to avoid clutter\n",
    "    if t_idx == 0:\n",
    "        print(f\"Transaction 1 Pairs processed: {len(pairs)}\")\n",
    "        print(f\"Example Pair from T1: {pairs[0]} -> Bucket {get_hash_bucket(pairs[0][0], pairs[0][1])}\")\n",
    "\n",
    "print(\"\\n--- SCAN COMPLETE ---\")\n",
    "\n",
    "# --- FILTERING L1 (Standard Part) ---\n",
    "# Filter items that meet MIN_SUPPORT\n",
    "L1 = {item: count for item, count in C1_counts.items() if count >= MIN_SUPPORT}\n",
    "sorted_L1 = sorted(L1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n[L1 Result] Frequent 1-Itemsets (Count >= {MIN_SUPPORT}):\")\n",
    "print(sorted_L1)\n",
    "\n",
    "# DHP BUCKET RESULTS: nothing pruned here. \n",
    "print(f\"\\n[DHP Result] Hash Table Status (Bucket Counts):\")\n",
    "print(f\"{'Bucket ID':<10} | {'Count':<10} | {'Status (Keep/Prune?)'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "valid_buckets = [] # To store buckets that passed the test\n",
    "for i in range(BUCKET_COUNT):\n",
    "    count = bucket_counts[i]\n",
    "    is_valid = count >= MIN_SUPPORT\n",
    "    status = \" KEEP\" if is_valid else \"PRUNE\"\n",
    "    if is_valid:\n",
    "        valid_buckets.append(i)\n",
    "        \n",
    "    print(f\"{i:<10} | {count:<10} | {status}\")\n",
    "\n",
    "print(f\"\\nValid Buckets for Next Step: {valid_buckets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec89aa93",
   "metadata": {
    "papermill": {
     "duration": 0.002976,
     "end_time": "2025-11-29T12:09:06.298489",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.295513",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Every bucket get keeped because of: B= 7. It's critical to save this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c1e3c2f0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:09:06.305706Z",
     "iopub.status.busy": "2025-11-29T12:09:06.305391Z",
     "iopub.status.idle": "2025-11-29T12:09:06.312909Z",
     "shell.execute_reply": "2025-11-29T12:09:06.311889Z"
    },
    "papermill": {
     "duration": 0.012793,
     "end_time": "2025-11-29T12:09:06.314268",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.301475",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERATING C2 CANDIDATES ---\n",
      "Items in L1: 7\n",
      "Valid Buckets: [0, 1, 2, 3, 4, 5, 6]\n",
      "Total possible pairs from L1: 21\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Candidates Generated (C2 Size): 21\n",
      "Candidates Pruned by DHP: 0\n"
     ]
    }
   ],
   "source": [
    "# L1 keys (items)\n",
    "l1_items = [x[0] for x in sorted_L1] \n",
    "\n",
    "print(f\"--- GENERATING C2 CANDIDATES ---\")\n",
    "print(f\"Items in L1: {len(l1_items)}\")\n",
    "print(f\"Valid Buckets: {valid_buckets}\")\n",
    "\n",
    "C2_candidates = []\n",
    "rejected_count = 0\n",
    "\n",
    "# Generate all pairs from L1 items\n",
    "possible_pairs = list(combinations(l1_items, 2))\n",
    "\n",
    "print(f\"Total possible pairs from L1: {len(possible_pairs)}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for pair in possible_pairs:\n",
    "    item1, item2 = pair\n",
    "    \n",
    "    # 1. Calculate Hash for this candidate\n",
    "    bucket = get_hash_bucket(item1, item2)\n",
    "    \n",
    "    # 2. DHP CHECK: Is this bucket in our valid list?\n",
    "    if bucket in valid_buckets:\n",
    "        C2_candidates.append(frozenset(pair))\n",
    "        # purely for display:\n",
    "        # print(f\"  Accepted: {pair} (Bucket {bucket})\") \n",
    "    else:\n",
    "        rejected_count += 1\n",
    "        print(f\"PRUNED by DHP: {pair} (Bucket {bucket})\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Candidates Generated (C2 Size): {len(C2_candidates)}\")\n",
    "print(f\"Candidates Pruned by DHP: {rejected_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c59e167",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:09:06.322209Z",
     "iopub.status.busy": "2025-11-29T12:09:06.321909Z",
     "iopub.status.idle": "2025-11-29T12:09:06.344797Z",
     "shell.execute_reply": "2025-11-29T12:09:06.343768Z"
    },
    "papermill": {
     "duration": 0.029017,
     "end_time": "2025-11-29T12:09:06.346407",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.317390",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Init Pass (Count L1 + Hash C2) ---\n",
      "L1 found: 7 items\n",
      "--- Step 2: Generate C2 (with DHP Pruning) ---\n",
      "C2 Candidates after DHP: 21 (out of 21 possible)\n",
      "--- Scanning DB for k=2 ---\n",
      "L2 found: 14 itemsets\n",
      "--- Scanning DB for k=3 ---\n",
      "L3 found: 12 itemsets\n",
      "--- Scanning DB for k=4 ---\n",
      "L4 found: 5 itemsets\n",
      "--- Scanning DB for k=5 ---\n",
      "L5 found: 1 itemsets\n",
      "\n",
      "==================================================\n",
      "FINAL OUTPUT\n",
      "==================================================\n",
      "Total Frequent Itemsets Found: 39\n",
      "Execution Time: 3.3021 ms\n",
      "Peak Memory Usage: 0.025233 MB\n",
      "--------------------------------------------------\n",
      "{Itemset: Support}\n",
      "{'Corn'}: 2\n",
      "{'Eggs'}: 4\n",
      "{'Kidney Beans'}: 5\n",
      "{'Milk'}: 3\n",
      "{'Nutmeg'}: 2\n",
      "{'Onion'}: 3\n",
      "{'Yogurt'}: 3\n",
      "{'Kidney Beans', 'Eggs'}: 4\n",
      "{'Milk', 'Eggs'}: 2\n",
      "{'Eggs', 'Nutmeg'}: 2\n",
      "{'Onion', 'Eggs'}: 3\n",
      "{'Eggs', 'Yogurt'}: 2\n",
      "{'Kidney Beans', 'Milk'}: 3\n",
      "{'Kidney Beans', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Onion'}: 3\n",
      "{'Kidney Beans', 'Yogurt'}: 3\n",
      "{'Milk', 'Yogurt'}: 2\n",
      "{'Onion', 'Nutmeg'}: 2\n",
      "{'Yogurt', 'Nutmeg'}: 2\n",
      "{'Onion', 'Yogurt'}: 2\n",
      "{'Kidney Beans', 'Corn'}: 2\n",
      "{'Kidney Beans', 'Milk', 'Eggs'}: 2\n",
      "{'Kidney Beans', 'Eggs', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Onion', 'Eggs'}: 3\n",
      "{'Kidney Beans', 'Eggs', 'Yogurt'}: 2\n",
      "{'Onion', 'Eggs', 'Nutmeg'}: 2\n",
      "{'Yogurt', 'Eggs', 'Nutmeg'}: 2\n",
      "{'Onion', 'Eggs', 'Yogurt'}: 2\n",
      "{'Kidney Beans', 'Milk', 'Yogurt'}: 2\n",
      "{'Kidney Beans', 'Onion', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Yogurt', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Onion', 'Yogurt'}: 2\n",
      "{'Onion', 'Yogurt', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Onion', 'Eggs', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Yogurt', 'Eggs', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Onion', 'Eggs', 'Yogurt'}: 2\n",
      "{'Yogurt', 'Onion', 'Eggs', 'Nutmeg'}: 2\n",
      "{'Kidney Beans', 'Onion', 'Yogurt', 'Nutmeg'}: 2\n",
      "{'Onion', 'Eggs', 'Nutmeg', 'Kidney Beans', 'Yogurt'}: 2\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    return [\n",
    "        frozenset(['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt']),\n",
    "        frozenset(['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt']),\n",
    "        frozenset(['Milk', 'Apple', 'Kidney Beans', 'Eggs']),\n",
    "        frozenset(['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt']),\n",
    "        frozenset(['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'])\n",
    "    ]\n",
    "\n",
    "def get_item_id(item_name):\n",
    "    \"\"\"Mapping items to integers for hashing.\"\"\"\n",
    "    mapping = {\n",
    "        'Apple': 1, 'Corn': 2, 'Dill': 3, 'Eggs': 4, \n",
    "        'Ice cream': 5, 'Kidney Beans': 6, 'Milk': 7, \n",
    "        'Nutmeg': 8, 'Onion': 9, 'Unicorn': 10, 'Yogurt': 11\n",
    "    }\n",
    "    return mapping.get(item_name, 0)\n",
    "\n",
    "def get_hash_bucket(item1, item2, bucket_count=7):\n",
    "    id1 = get_item_id(item1)\n",
    "    id2 = get_item_id(item2)\n",
    "    # Ensuring order doesn't change hash (commutative)\n",
    "    if id1 > id2: id1, id2 = id2, id1\n",
    "    return (id1 * 10 + id2) % bucket_count\n",
    "\n",
    "# --- 2. Core Functions ---\n",
    "\n",
    "def apriori_gen(Lk_minus_1, k):\n",
    "    \"\"\"\n",
    "    Generates Ck from L(k-1).\n",
    "    For k=2, we use simple combinations.\n",
    "    For k>2, we join sets that share k-2 items.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    len_Lk = len(Lk_minus_1)\n",
    "    lk_list = list(Lk_minus_1)\n",
    "    \n",
    "    if k == 2:\n",
    "        # Simple pairs\n",
    "        return list(combinations(lk_list, 2))\n",
    "    \n",
    "    # Standard Apriori Join for k > 2\n",
    "    for i in range(len_Lk):\n",
    "        for j in range(i + 1, len_Lk):\n",
    "            L1 = list(lk_list[i])\n",
    "            L2 = list(lk_list[j])\n",
    "            L1.sort(); L2.sort()\n",
    "            \n",
    "            # If first k-2 elements are equal, join them\n",
    "            if L1[:k-2] == L2[:k-2]:\n",
    "                candidates.append(lk_list[i] | lk_list[j])\n",
    "                \n",
    "    return candidates\n",
    "\n",
    "def scan_and_count(dataset, candidates):\n",
    "    \"\"\"Counts support for a list of candidates.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for tid in dataset:\n",
    "        for cand in candidates:\n",
    "            # cand can be a tuple (from combinations) or frozenset\n",
    "            cand_set = frozenset(cand)\n",
    "            if cand_set.issubset(tid):\n",
    "                counts[cand_set] += 1\n",
    "    return counts\n",
    "\n",
    "# --- 3. Main Algorithm with Metrics ---\n",
    "\n",
    "def run_apriori_dhp(min_support=2):\n",
    "    # Start Metrics\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataset = load_dataset()\n",
    "    bucket_count = 7\n",
    "    \n",
    "    global_frequent_itemsets = {} # Final Result Storage\n",
    "    \n",
    "    print(\"--- Step 1: Init Pass (Count L1 + Hash C2) ---\")\n",
    "    c1_counts = defaultdict(int)\n",
    "    bucket_counts = defaultdict(int)\n",
    "    \n",
    "    for transaction in dataset:\n",
    "        # Count Items\n",
    "        for item in transaction:\n",
    "            c1_counts[item] += 1\n",
    "            \n",
    "        # Hash Pairs (DHP)\n",
    "        items = list(transaction)\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                b_idx = get_hash_bucket(items[i], items[j], bucket_count)\n",
    "                bucket_counts[b_idx] += 1\n",
    "\n",
    "    # Generate L1\n",
    "    L1 = [item for item, count in c1_counts.items() if count >= min_support]\n",
    "    L1.sort() # Sorting helps deterministic behavior\n",
    "    \n",
    "    # Store L1\n",
    "    for item in L1:\n",
    "        global_frequent_itemsets[frozenset([item])] = c1_counts[item]\n",
    "\n",
    "    print(f\"L1 found: {len(L1)} items\")\n",
    "    \n",
    "    # --- Step 2: Generate C2 using DHP Filter ---\n",
    "    print(\"--- Step 2: Generate C2 (with DHP Pruning) ---\")\n",
    "    valid_buckets = {b for b, cnt in bucket_counts.items() if cnt >= min_support}\n",
    "    \n",
    "    C2_candidates = []\n",
    "    l1_pairs = list(combinations(L1, 2))\n",
    "    \n",
    "    for pair in l1_pairs:\n",
    "        b_idx = get_hash_bucket(pair[0], pair[1], bucket_count)\n",
    "        if b_idx in valid_buckets:\n",
    "            C2_candidates.append(frozenset(pair))\n",
    "    \n",
    "    print(f\"C2 Candidates after DHP: {len(C2_candidates)} (out of {len(l1_pairs)} possible)\")\n",
    "    \n",
    "    # Loop Variables\n",
    "    current_C = C2_candidates\n",
    "    k = 2\n",
    "    \n",
    "    # --- Step 3: Iterative Loop (L2, L3...) ---\n",
    "    while len(current_C) > 0:\n",
    "        print(f\"--- Scanning DB for k={k} ---\")\n",
    "        \n",
    "        # Count Support\n",
    "        candidates_counts = scan_and_count(dataset, current_C)\n",
    "        \n",
    "        # Filter L_k\n",
    "        L_k = []\n",
    "        for cand, count in candidates_counts.items():\n",
    "            if count >= min_support:\n",
    "                L_k.append(cand)\n",
    "                global_frequent_itemsets[cand] = count\n",
    "        \n",
    "        print(f\"L{k} found: {len(L_k)} itemsets\")\n",
    "        \n",
    "        if len(L_k) <= 1:\n",
    "            break # Cannot generate C_(k+1) from 1 or 0 items\n",
    "            \n",
    "        # Generate Next Candidates (C_k+1) - Standard Join\n",
    "        k += 1\n",
    "        current_C = apriori_gen(L_k, k)\n",
    "        \n",
    "    # End Metrics\n",
    "    end_time = time.time()\n",
    "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    execution_time = (end_time - start_time) * 1000 # ms\n",
    "    peak_mem_mb = peak_mem / (1024 * 1024)\n",
    "    \n",
    "    return global_frequent_itemsets, execution_time, peak_mem_mb\n",
    "\n",
    "# --- 4. Execution ---\n",
    "\n",
    "final_results, exec_time, peak_memory = run_apriori_dhp(min_support=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL OUTPUT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Frequent Itemsets Found: {len(final_results)}\")\n",
    "print(f\"Execution Time: {exec_time:.4f} ms\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.6f} MB\")\n",
    "print(\"-\" * 50)\n",
    "print(\"{Itemset: Support}\")\n",
    "for itemset, support in final_results.items():\n",
    "    # formatting frozenset to look cleaner\n",
    "    clean_set = set(itemset) \n",
    "    print(f\"{clean_set}: {support}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08aa08e6",
   "metadata": {
    "papermill": {
     "duration": 0.002999,
     "end_time": "2025-11-29T12:09:06.352664",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.349665",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36b54947",
   "metadata": {
    "papermill": {
     "duration": 0.002883,
     "end_time": "2025-11-29T12:09:06.358673",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.355790",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family: 'Segoe UI', Roboto, Helvetica, Arial, sans-serif; line-height: 1.6; color: #e0e0e0; background-color: #121212; padding: 20px; border-radius: 8px;\">\n",
    "\n",
    " <div style=\"background-color: #1f1f1f; border-left: 5px solid #bb86fc; padding: 15px; margin-bottom: 20px; border-radius: 4px; box-shadow: 0 4px 6px rgba(0,0,0,0.3);\">\n",
    "        <h1 style=\"margin:0; color: #ffffff; font-weight: 300; letter-spacing: 1px;\">Sampling Technique in Association Rule Mining</h1>\n",
    "        <p style=\"margin-top: 5px; font-size: 1.1em; color: #b0b0b0;\">A Data-Driven Optimization Strategy</p>\n",
    "    </div>\n",
    "\n",
    "<p style=\"color: #cccccc;\">\n",
    "        While algorithms like <strong style=\"color: #bb86fc;\">DHP (Direct Hashing and Pruning)</strong> optimize <i>how</i> we process data, <strong style=\"color: #03dac6;\">Sampling</strong> optimizes <i>what</i> data we process. \n",
    "        In massive datasets (e.g., millions of transactions), running a full Apriori scan is computationally expensive. Sampling addresses this by analyzing a random subset of data and generalizing the results.\n",
    "    </p>\n",
    "\n",
    "<div style=\"background-color: #2c2c2c; border: 1px solid #cf6679; border-radius: 5px; padding: 15px; margin: 25px 0;\">\n",
    "        <h3 style=\"margin-top: 0; color: #ff8a80;\">‚ö° The Core Challenge: Threshold Adjustment</h3>\n",
    "        <p>\n",
    "            When you reduce the dataset size, you cannot simply use the same absolute support count. More importantly, you face the <b style=\"color: #ffcc80;\">Variance Problem</b>: A frequent item might accidentally appear less frequently in your random sample than in the full DB.\n",
    "        </p>\n",
    "        <p><b style=\"color: #81c784;\">The Solution: Lowered Support Threshold</b></p>\n",
    "        <p>We apply a \"Safety Margin\" ($1 - \\epsilon$) to the support threshold to avoid False Negatives.</p>\n",
    "        \n",
    "<div style=\"background-color: #121212; padding: 15px; border-radius: 4px; text-align: center; font-family: 'Courier New', monospace; font-weight: bold; border: 1px solid #444; color: #80cbc4;\">\n",
    "            $$ S_{sample} = (S_{db\\_fraction} \\times N_{sample}) \\times (1 - \\epsilon) $$\n",
    "        </div>\n",
    "        <ul style=\"font-size: 0.9em; color: #b0b0b0; margin-top: 10px;\">\n",
    "            <li><span style=\"color:#80cbc4\">$S_{sample}$</span>: New Minimum Support Count for the sample.</li>\n",
    "            <li><span style=\"color:#80cbc4\">$S_{db\\_fraction}$</span>: The target support percentage (e.g., 0.01 for 1%).</li>\n",
    "            <li><span style=\"color:#80cbc4\">$\\epsilon$</span>: The relaxation factor (e.g., 0.1 for a 10% safety buffer).</li>\n",
    "        </ul>\n",
    "    </div>\n",
    "\n",
    "<h3 style=\"color: #03dac6; border-bottom: 1px solid #333; padding-bottom: 5px;\">Step-by-Step Algorithm</h3>\n",
    "    <ol style=\"color: #cccccc;\">\n",
    "        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Select Sample:</b> Pick a random subset of size $M$ from the original database $N$.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Adjust Threshold:</b> Calculate the new support using the safety margin formula.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Run Apriori:</b> Execute the standard (or DHP) Apriori on the sample $M$.</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Identify Candidates:</b> The output is considered the \"Global Candidate Set\".</li>\n",
    "        <li style=\"margin-bottom: 8px;\"><b style=\"color: #fff;\">Verify (Optional):</b> Run one final scan on the full $N$ to verify the actual counts of these candidates (eliminating False Positives).</li>\n",
    "    </ol>\n",
    "\n",
    "\n",
    "\n",
    " <h3 style=\"color: #03dac6; border-bottom: 1px solid #333; padding-bottom: 5px; margin-top: 30px;\">Pros & Cons Analysis</h3>\n",
    "    <table style=\"width: 100%; border-collapse: collapse; margin-top: 10px; border: 1px solid #444;\">\n",
    "        <thead>\n",
    "            <tr style=\"background-color: #333;\">\n",
    "                <th style=\"padding: 12px; border: 1px solid #444; text-align: left; width: 20%; color: #ffffff;\">Feature</th>\n",
    "                <th style=\"padding: 12px; border: 1px solid #444; text-align: left; color: #81c784;\">Pros (Advantages)</th>\n",
    "                <th style=\"padding: 12px; border: 1px solid #444; text-align: left; color: #e57373;\">Cons (Risks)</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr style=\"background-color: #1a1a1a;\">\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; font-weight: bold; color: #eee;\">Speed</td>\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">üöÄ <b>Extremely Fast</b>. Reducing data by 90% can speed up execution by 100x due to combinatorial explosion.</td>\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">Overhead of sampling if data is not easily accessible (e.g., on disk).</td>\n",
    "            </tr>\n",
    "            <tr style=\"background-color: #222;\">\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; font-weight: bold; color: #eee;\">Memory</td>\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">‚úÖ <b>High Efficiency</b>. Allows processing terabyte-scale DBs on standard RAM.</td>\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">None.</td>\n",
    "            </tr>\n",
    "            <tr style=\"background-color: #1a1a1a;\">\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; font-weight: bold; color: #eee;\">Accuracy</td>\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">Good enough for finding \"Strong\" rules.</td>\n",
    "                <td style=\"padding: 12px; border: 1px solid #444; color: #ccc;\">‚ö†Ô∏è <b>False Negatives</b>. May miss rare but significant patterns if the sample is unrepresentative.</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    " <div style=\"margin-top: 30px; padding: 15px; border-top: 1px solid #333; font-size: 0.9em; color: #888; font-style: italic;\">\n",
    "        Note: This sampling logic is the foundational concept behind the <b style=\"color: #ccc;\">SON Algorithm</b> (Savasere, Omiecinski, and Navathe), which utilizes MapReduce for large-scale mining.\n",
    "    </div>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5061c119",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T12:09:06.366662Z",
     "iopub.status.busy": "2025-11-29T12:09:06.366381Z",
     "iopub.status.idle": "2025-11-29T12:09:06.393685Z",
     "shell.execute_reply": "2025-11-29T12:09:06.392733Z"
    },
    "papermill": {
     "duration": 0.033394,
     "end_time": "2025-11-29T12:09:06.395076",
     "exception": false,
     "start_time": "2025-11-29T12:09:06.361682",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Sampling ---\n",
      "Total Data Size: 5\n",
      "Sample Size: 3\n",
      "Selected Sample Transactions:\n",
      "  1: ['Onion', 'Eggs', 'Nutmeg', 'Milk', 'Kidney Beans', 'Yogurt']\n",
      "  2: ['Onion', 'Eggs', 'Ice cream', 'Corn', 'Kidney Beans']\n",
      "  3: ['Kidney Beans', 'Apple', 'Milk', 'Eggs']\n",
      "\n",
      "--- Step 2: Threshold Adjustment ---\n",
      "Global Desired Support: 40.0%\n",
      "Theoretical Count in Sample: 1.20\n",
      "Adjusted Min Support for Sample: 1\n",
      "\n",
      "--- Step 3: Running Apriori on Sample ---\n",
      "Found 95 potential candidates in the sample.\n",
      "Example Candidates found in sample: [frozenset({'Onion'}), frozenset({'Eggs'}), frozenset({'Nutmeg'})] ...\n",
      "\n",
      "--- Step 4: Global Verification ---\n",
      "Final Verified Frequent Itemsets: 39\n",
      "Global Min Support Count Needed: 2\n",
      "\n",
      "Top 5 Verified Rules:\n",
      "  {'Kidney Beans'} : 5\n",
      "  {'Eggs'} : 4\n",
      "  {'Kidney Beans', 'Eggs'} : 4\n",
      "  {'Onion'} : 3\n",
      "  {'Milk'} : 3\n",
      "\n",
      "=========================================\n",
      "Execution Metrics:\n",
      "  Execution Time:     11.2003 ms\n",
      "  Peak Memory Usage:  0.063807 MB\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import time\n",
    "import tracemalloc\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "tracemalloc.start()\n",
    "start_time = time.process_time()\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 1. Preparing Dataset\n",
    "# ---------------------------------------------------------\n",
    "raw_data = [\n",
    "    ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "    ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "    ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs']\n",
    "]\n",
    "\n",
    "# Removed iterated samples and efficiency (Convert to frozenset)\n",
    "dataset = [frozenset(t) for t in raw_data]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 2. Standard Apriori Function\n",
    "# ---------------------------------------------------------\n",
    "def get_frequent_itemsets(data_subset, min_support_count):\n",
    "    item_counts = defaultdict(int)\n",
    "    \n",
    "    # Count initial items (C1)\n",
    "    for transaction in data_subset:\n",
    "        for item in transaction:\n",
    "            item_counts[frozenset([item])] += 1\n",
    "    \n",
    "    # Filter L1 (Using Loop instead of Comprehension)\n",
    "    L1 = {}\n",
    "    for item, count in item_counts.items():\n",
    "        if count >= min_support_count:\n",
    "            L1[item] = count\n",
    "   \n",
    "    current_L = L1\n",
    "    all_frequent_itemsets = L1.copy()\n",
    "    k = 2\n",
    "    \n",
    "    while current_L:\n",
    "        # Generate Candidates (Ck)\n",
    "        candidates = set()\n",
    "        prev_items = list(current_L.keys())\n",
    "        for i in range(len(prev_items)):\n",
    "            for j in range(i + 1, len(prev_items)):\n",
    "                union_set = prev_items[i] | prev_items[j]\n",
    "                if len(union_set) == k:\n",
    "                    candidates.add(union_set)\n",
    "        \n",
    "        # Count Ck\n",
    "        ck_counts = defaultdict(int)\n",
    "        for transaction in data_subset:\n",
    "            for candidate in candidates:\n",
    "                if candidate.issubset(transaction):\n",
    "                    ck_counts[candidate] += 1\n",
    "                    \n",
    "        # Filter Lk (Using Loop instead of Comprehension)\n",
    "        Lk = {}\n",
    "        for item, count in ck_counts.items():\n",
    "            if count >= min_support_count:\n",
    "                Lk[item] = count\n",
    "        \n",
    "        if not Lk:\n",
    "            break\n",
    "            \n",
    "        all_frequent_itemsets.update(Lk)\n",
    "        current_L = Lk\n",
    "        k += 1\n",
    "        \n",
    "    return all_frequent_itemsets\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 3. Sampling Logic Execution\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "GLOBAL_MIN_SUPPORT_RATIO = 0.4 \n",
    "SAMPLE_SIZE = 3                 \n",
    "SAFETY_MARGIN = 0.9    \n",
    "\n",
    "random.seed(42) \n",
    "\n",
    "sample_data = random.sample(dataset, SAMPLE_SIZE)\n",
    "\n",
    "print(f\"--- Step 1: Sampling ---\")\n",
    "print(f\"Total Data Size: {len(dataset)}\")\n",
    "print(f\"Sample Size: {len(sample_data)}\")\n",
    "print(\"Selected Sample Transactions:\")\n",
    "for i, t in enumerate(sample_data):\n",
    "    print(f\"  {i+1}: {list(t)}\")\n",
    "\n",
    "theoretical_count = GLOBAL_MIN_SUPPORT_RATIO * len(sample_data) # 0.4 * 3 = 1.2\n",
    "adjusted_support = int(theoretical_count * SAFETY_MARGIN)       # 1.2 * 0.9 = 1.08 -> int(1.08) = 1\n",
    "\n",
    "sample_min_support = max(1, adjusted_support)\n",
    "\n",
    "print(f\"\\n--- Step 2: Threshold Adjustment ---\")\n",
    "print(f\"Global Desired Support: {GLOBAL_MIN_SUPPORT_RATIO*100}%\")\n",
    "print(f\"Theoretical Count in Sample: {theoretical_count:.2f}\")\n",
    "print(f\"Adjusted Min Support for Sample: {sample_min_support}\")\n",
    "\n",
    "print(f\"\\n--- Step 3: Running Apriori on Sample ---\")\n",
    "local_candidates = get_frequent_itemsets(sample_data, sample_min_support)\n",
    "print(f\"Found {len(local_candidates)} potential candidates in the sample.\")\n",
    "\n",
    "print(\"Example Candidates found in sample:\", list(local_candidates.keys())[:3], \"...\")\n",
    "\n",
    "print(f\"\\n--- Step 4: Global Verification ---\")\n",
    "\n",
    "final_verified_itemsets = {}\n",
    "global_min_count = int(GLOBAL_MIN_SUPPORT_RATIO * len(dataset)) # 0.4 * 5 = 2\n",
    "\n",
    "# Verify candidates against full dataset\n",
    "for candidate in local_candidates.keys():\n",
    "    actual_count = 0\n",
    "    for transaction in dataset:\n",
    "        if candidate.issubset(transaction):\n",
    "            actual_count += 1\n",
    "            \n",
    "    if actual_count >= global_min_count:\n",
    "        final_verified_itemsets[candidate] = actual_count\n",
    "\n",
    "print(f\"Final Verified Frequent Itemsets: {len(final_verified_itemsets)}\")\n",
    "print(f\"Global Min Support Count Needed: {global_min_count}\")\n",
    "\n",
    "sorted_results = sorted(final_verified_itemsets.items(), key=lambda x: x[1], reverse=True)\n",
    "print(\"\\nTop 5 Verified Rules:\")\n",
    "for itemset, count in sorted_results[:5]:\n",
    "    print(f\"  {set(itemset)} : {count}\")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# 4. Metrics Calculation (Time & Memory)\n",
    "# ---------------------------------------------------------\n",
    "end_time = time.process_time()\n",
    "current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
    "tracemalloc.stop()\n",
    "\n",
    "execution_time = (end_time - start_time) * 1000 # convert to ms\n",
    "peak_memory_mb = peak_mem / (1024 * 1024)       # convert bytes to MB\n",
    "\n",
    "print(f\"\\n=========================================\")\n",
    "print(f\"Execution Metrics:\")\n",
    "print(f\"  Execution Time:     {execution_time:.4f} ms\")\n",
    "print(f\"  Peak Memory Usage:  {peak_memory_mb:.6f} MB\")\n",
    "print(f\"=========================================\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.248032,
   "end_time": "2025-11-29T12:09:06.916921",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T12:08:59.668889",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
