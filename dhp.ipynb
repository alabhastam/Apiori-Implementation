{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56baf85b",
   "metadata": {
    "papermill": {
     "duration": 0.002991,
     "end_time": "2025-11-29T10:07:31.179931",
     "exception": false,
     "start_time": "2025-11-29T10:07:31.176940",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"\n",
    "    background-color: #1e1e1e; \n",
    "    color: #e0e0e0; \n",
    "    font-family: 'Consolas', 'Monaco', 'Courier New', monospace; \n",
    "    padding: 25px; \n",
    "    border-radius: 8px; \n",
    "    border: 1px solid #333; \n",
    "    box-shadow: 0 4px 15px rgba(0,0,0,0.5);\n",
    "    max-width: 950px;\">\n",
    "\n",
    "<h1 style=\"\n",
    "        color: #4fc3f7; \n",
    "        border-bottom: 2px solid #0288d1; \n",
    "        padding-bottom: 10px; \n",
    "        margin-top: 0;\n",
    "        font-family: 'Segoe UI', sans-serif;\n",
    "        letter-spacing: 1px;\">\n",
    "        &lt;Optimization: Direct Hashing & Pruning (DHP) /&gt;\n",
    "    </h1>\n",
    "\n",
    " <p style=\"font-size: 1.05em; color: #b0bec5;\">\n",
    "        The standard Apriori algorithm faces a critical bottleneck during the generation of \n",
    "        <strong style=\"color: #fff;\">2-itemsets (C‚ÇÇ)</strong>. DHP addresses this by aggressively pruning candidate pairs early using a hash-based filtering technique.\n",
    "    </p>\n",
    "\n",
    "<div style=\"\n",
    "        background-color: #263238; \n",
    "        border-left: 5px solid #ff5252; \n",
    "        padding: 15px; \n",
    "        margin: 25px 0; \n",
    "        border-radius: 4px;\">\n",
    "        <h3 style=\"margin-top: 0; color: #ff8a80; font-family: 'Segoe UI', sans-serif;\">\n",
    "            ‚ö†Ô∏è The Bottleneck: Combinatorial Explosion\n",
    "        </h3>\n",
    "        <p style=\"margin-bottom: 0; font-size: 0.95em;\">\n",
    "            If you have 1,000 frequent items in <code>L‚ÇÅ</code>, standard Apriori must generate \n",
    "            <code style=\"color: #ff5252; background-color: rgba(255,255,255,0.1); padding: 2px 6px; border-radius: 4px;\">C(1000, 2) ‚âà 500,000</code> \n",
    "            candidate pairs. Checking all these against the database is slow and memory-intensive.\n",
    "        </p>\n",
    "    </div>\n",
    "\n",
    "<h2 style=\"color: #81d4fa; font-family: 'Segoe UI', sans-serif;\">// How DHP Works</h2>\n",
    "    <p>\n",
    "        DHP <strong>\"hacks\" the first pass</strong>. While counting single items (k=1), it simultaneously gathers data about pairs.\n",
    "    </p>\n",
    "\n",
    "<div style=\"margin-left: 10px;\">\n",
    "        <div style=\"margin-bottom: 15px;\">\n",
    "            <strong style=\"color: #fff; font-size: 1.1em;\">1. The Hash Function</strong><br>\n",
    "            During the initial scan, every pair of items in a transaction is passed through a function:\n",
    "            <div style=\"\n",
    "                background-color: #000; \n",
    "                color: #a5d6a7; \n",
    "                padding: 10px; \n",
    "                border-radius: 4px; \n",
    "                margin: 10px 0; \n",
    "                border-left: 3px solid #66bb6a;\">\n",
    "                bucket_index = ( (order(x) * 10) + order(y) ) % N\n",
    "            </div>\n",
    "        </div>\n",
    "\n",
    " <div style=\"margin-bottom: 15px;\">\n",
    "            <strong style=\"color: #fff; font-size: 1.1em;\">2. The Bucket Count</strong><br>\n",
    "            A Hash Table tracks counts. If a pair maps to bucket #5, we increment bucket #5. We don't store <em>which</em> pair it was, just the frequency.\n",
    "        </div>\n",
    "\n",
    "<div>\n",
    "            <strong style=\"color: #fff; font-size: 1.1em;\">3. The Golden Rule (Pruning)</strong><br>\n",
    "            After the scan, we check the buckets. <br>\n",
    "            <span style=\"color: #ffcc80;\">If a bucket's count < min_support, ALL pairs mapping to that bucket are discarded immediately.</span>\n",
    "        </div>\n",
    "    </div>\n",
    "\n",
    "<div style=\"\n",
    "        background-color: #1b5e20; \n",
    "        background: linear-gradient(145deg, #1b5e20 0%, #2e7d32 100%);\n",
    "        padding: 20px; \n",
    "        margin: 30px 0; \n",
    "        border-radius: 6px; \n",
    "        color: #e8f5e9;\">\n",
    "        <h3 style=\"margin-top: 0; color: #fff; font-family: 'Segoe UI', sans-serif;\">üí° Concrete Example</h3>\n",
    "        <p style=\"margin: 5px 0;\"><strong>Scenario:</strong> <code>min_support = 10</code></p>\n",
    "        <ul style=\"list-style-type: square; padding-left: 20px;\">\n",
    "            <li>We analyze pair <code>{Milk, Bread}</code>.</li>\n",
    "            <li>Hash function maps it to <strong>Bucket #5</strong>.</li>\n",
    "            <li>Total count in Bucket #5 is found to be <strong>8</strong>.</li>\n",
    "        </ul>\n",
    "        <hr style=\"border: 0; border-top: 1px solid rgba(255,255,255,0.3); margin: 10px 0;\">\n",
    "        <p style=\"margin-bottom: 0; font-weight: bold;\">\n",
    "            Conclusion: Even if all 8 hits were {Milk, Bread}, 8 < 10. This pair is impossible. DROP IT.\n",
    "        </p>\n",
    "    </div>\n",
    "\n",
    "<h2 style=\"color: #81d4fa; font-family: 'Segoe UI', sans-serif;\">// Performance Comparison</h2>\n",
    "    \n",
    " <table style=\"width: 100%; border-collapse: collapse; margin-top: 15px; font-size: 0.95em;\">\n",
    "        <thead>\n",
    "            <tr style=\"border-bottom: 2px solid #4fc3f7;\">\n",
    "                <th style=\"text-align: left; padding: 12px; color: #4fc3f7;\">Feature</th>\n",
    "                <th style=\"text-align: left; padding: 12px; color: #b0bec5;\">Standard Apriori</th>\n",
    "                <th style=\"text-align: left; padding: 12px; color: #fff;\">Apriori + DHP</th>\n",
    "            </tr>\n",
    "        </thead>\n",
    "        <tbody>\n",
    "            <tr style=\"border-bottom: 1px solid #424242;\">\n",
    "                <td style=\"padding: 12px; color: #e0e0e0;\"><strong>Candidate Gen (C‚ÇÇ)</strong></td>\n",
    "                <td style=\"padding: 12px; color: #9e9e9e;\">Blindly joins all L‚ÇÅ items ($\\approx n^2/2$).</td>\n",
    "                <td style=\"padding: 12px; color: #81c784;\">Only generates pairs from frequent buckets.</td>\n",
    "            </tr>\n",
    "            <tr style=\"border-bottom: 1px solid #424242;\">\n",
    "                <td style=\"padding: 12px; color: #e0e0e0;\"><strong>DB Size</strong></td>\n",
    "                <td style=\"padding: 12px; color: #9e9e9e;\">Full scan every time.</td>\n",
    "                <td style=\"padding: 12px; color: #81c784;\">Can \"trim\" DB by removing useless transactions.</td>\n",
    "            </tr>\n",
    "        </tbody>\n",
    "    </table>\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "135188ad",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T10:07:31.186435Z",
     "iopub.status.busy": "2025-11-29T10:07:31.186108Z",
     "iopub.status.idle": "2025-11-29T10:07:33.101413Z",
     "shell.execute_reply": "2025-11-29T10:07:33.100440Z"
    },
    "papermill": {
     "duration": 1.92052,
     "end_time": "2025-11-29T10:07:33.103099",
     "exception": false,
     "start_time": "2025-11-29T10:07:31.182579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from itertools import combinations\n",
    "import tracemalloc\n",
    "import time\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "55f75461",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T10:07:33.109641Z",
     "iopub.status.busy": "2025-11-29T10:07:33.109259Z",
     "iopub.status.idle": "2025-11-29T10:07:33.119151Z",
     "shell.execute_reply": "2025-11-29T10:07:33.117997Z"
    },
    "papermill": {
     "duration": 0.015052,
     "end_time": "2025-11-29T10:07:33.120690",
     "exception": false,
     "start_time": "2025-11-29T10:07:33.105638",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CONFIGURATION ---\n",
      "Transaction Count: 5\n",
      "Min Support: 2\n",
      "Hash Buckets: 7\n",
      "------------------------------\n",
      "Item ID Mapping (for Hashing):\n",
      "  Apple: 1\n",
      "  Corn: 2\n",
      "  Dill: 3\n",
      "  Eggs: 4\n",
      "  Ice cream: 5\n",
      "  Kidney Beans: 6\n",
      "  Milk: 7\n",
      "  Nutmeg: 8\n",
      "  Onion: 9\n",
      "  Unicorn: 10\n",
      "  Yogurt: 11\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# The Raw Data\n",
    "raw_data = [\n",
    "    ['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt'],\n",
    "    ['Milk', 'Apple', 'Kidney Beans', 'Eggs'],\n",
    "    ['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt'],\n",
    "    ['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'] # Note duplicates\n",
    "]\n",
    "\n",
    "# Convert to list of sets (removes duplicate 'Onion' in last row)\n",
    "dataset = [set(transaction) for transaction in raw_data]\n",
    "\n",
    "# 2. Parameters\n",
    "MIN_SUPPORT = 2\n",
    "BUCKET_COUNT = 7  \n",
    "\n",
    "# Create a Mapping (String -> Integer) for the Hash Function\n",
    "# We sort all unique items alphabetically to ensure deterministic IDs\n",
    "unique_items = sorted(set(item for sublist in dataset for item in sublist))\n",
    "item_to_id = {item: i+1 for i, item in enumerate(unique_items)}\n",
    "\n",
    "# Define the Hash Function\n",
    "def get_hash_bucket(item1, item2, bucket_count=BUCKET_COUNT):\n",
    "    \n",
    "    # Get IDs\n",
    "    id1 = item_to_id[item1]\n",
    "    id2 = item_to_id[item2]\n",
    "    \n",
    "    # Ensure order to make hash commutative: hash(A, B) == hash(B, A)\n",
    "    first = min(id1, id2)\n",
    "    second = max(id1, id2)\n",
    "    \n",
    "    # Calculate Hash\n",
    "    val = (first * 10) + second\n",
    "    return val % bucket_count\n",
    "\n",
    "# --- DISPLAY SETUP ---\n",
    "print(f\"--- CONFIGURATION ---\")\n",
    "print(f\"Transaction Count: {len(dataset)}\")\n",
    "print(f\"Min Support: {MIN_SUPPORT}\")\n",
    "print(f\"Hash Buckets: {BUCKET_COUNT}\")\n",
    "print(\"-\" * 30)\n",
    "print(\"Item ID Mapping (for Hashing):\")\n",
    "for item, idx in item_to_id.items():\n",
    "    print(f\"  {item}: {idx}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f3c78d7d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T10:07:33.126837Z",
     "iopub.status.busy": "2025-11-29T10:07:33.126536Z",
     "iopub.status.idle": "2025-11-29T10:07:33.136165Z",
     "shell.execute_reply": "2025-11-29T10:07:33.135124Z"
    },
    "papermill": {
     "duration": 0.014325,
     "end_time": "2025-11-29T10:07:33.137556",
     "exception": false,
     "start_time": "2025-11-29T10:07:33.123231",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STARTING SCAN 1\n",
      "Transaction 1 Pairs processed: 15\n",
      "Example Pair from T1: ('Nutmeg', 'Kidney Beans') -> Bucket 5\n",
      "\n",
      "--- SCAN COMPLETE ---\n",
      "\n",
      "[L1 Result] Frequent 1-Itemsets (Count >= 2):\n",
      "[('Kidney Beans', 5), ('Eggs', 4), ('Onion', 3), ('Milk', 3), ('Yogurt', 3), ('Nutmeg', 2), ('Corn', 2)]\n",
      "\n",
      "[DHP Result] Hash Table Status (Bucket Counts):\n",
      "Bucket ID  | Count      | Status (Keep/Prune?)\n",
      "---------------------------------------------\n",
      "0          | 8          |  KEEP\n",
      "1          | 6          |  KEEP\n",
      "2          | 5          |  KEEP\n",
      "3          | 9          |  KEEP\n",
      "4          | 11         |  KEEP\n",
      "5          | 8          |  KEEP\n",
      "6          | 9          |  KEEP\n",
      "\n",
      "Valid Buckets for Next Step: [0, 1, 2, 3, 4, 5, 6]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Initialization\n",
    "C1_counts = defaultdict(int)\n",
    "bucket_counts = defaultdict(int) # This acts as our Hash Table\n",
    "\n",
    "print(\"STARTING SCAN 1\")\n",
    "\n",
    "for t_idx, transaction in enumerate(dataset):\n",
    "    # though sets are fine for combinations\n",
    "    items = list(transaction)\n",
    "    \n",
    "    for item in items:\n",
    "        C1_counts[item] += 1\n",
    "        \n",
    "    #  DHP Special: Hash all 2-item subsets\n",
    "    # Generate all pairs (combinations of size 2)\n",
    "    pairs = list(combinations(items, 2))\n",
    "    \n",
    "    for pair in pairs:\n",
    "        bucket_idx = get_hash_bucket(pair[0], pair[1])\n",
    "        bucket_counts[bucket_idx] += 1\n",
    "        \n",
    "    # Optional: Print detail for first transaction only to avoid clutter\n",
    "    if t_idx == 0:\n",
    "        print(f\"Transaction 1 Pairs processed: {len(pairs)}\")\n",
    "        print(f\"Example Pair from T1: {pairs[0]} -> Bucket {get_hash_bucket(pairs[0][0], pairs[0][1])}\")\n",
    "\n",
    "print(\"\\n--- SCAN COMPLETE ---\")\n",
    "\n",
    "# --- FILTERING L1 (Standard Part) ---\n",
    "# Filter items that meet MIN_SUPPORT\n",
    "L1 = {item: count for item, count in C1_counts.items() if count >= MIN_SUPPORT}\n",
    "sorted_L1 = sorted(L1.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(f\"\\n[L1 Result] Frequent 1-Itemsets (Count >= {MIN_SUPPORT}):\")\n",
    "print(sorted_L1)\n",
    "\n",
    "# DHP BUCKET RESULTS: nothing pruned here. \n",
    "print(f\"\\n[DHP Result] Hash Table Status (Bucket Counts):\")\n",
    "print(f\"{'Bucket ID':<10} | {'Count':<10} | {'Status (Keep/Prune?)'}\")\n",
    "print(\"-\" * 45)\n",
    "\n",
    "valid_buckets = [] # To store buckets that passed the test\n",
    "for i in range(BUCKET_COUNT):\n",
    "    count = bucket_counts[i]\n",
    "    is_valid = count >= MIN_SUPPORT\n",
    "    status = \" KEEP\" if is_valid else \"PRUNE\"\n",
    "    if is_valid:\n",
    "        valid_buckets.append(i)\n",
    "        \n",
    "    print(f\"{i:<10} | {count:<10} | {status}\")\n",
    "\n",
    "print(f\"\\nValid Buckets for Next Step: {valid_buckets}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fb7c506",
   "metadata": {
    "papermill": {
     "duration": 0.002345,
     "end_time": "2025-11-29T10:07:33.142416",
     "exception": false,
     "start_time": "2025-11-29T10:07:33.140071",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Every bucket get keeped because of: B= 7. It's critical to save this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de69a190",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T10:07:33.148410Z",
     "iopub.status.busy": "2025-11-29T10:07:33.148014Z",
     "iopub.status.idle": "2025-11-29T10:07:33.155863Z",
     "shell.execute_reply": "2025-11-29T10:07:33.154854Z"
    },
    "papermill": {
     "duration": 0.012695,
     "end_time": "2025-11-29T10:07:33.157288",
     "exception": false,
     "start_time": "2025-11-29T10:07:33.144593",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- GENERATING C2 CANDIDATES ---\n",
      "Items in L1: 7\n",
      "Valid Buckets: [0, 1, 2, 3, 4, 5, 6]\n",
      "Total possible pairs from L1: 21\n",
      "----------------------------------------\n",
      "----------------------------------------\n",
      "Candidates Generated (C2 Size): 21\n",
      "Candidates Pruned by DHP: 0\n"
     ]
    }
   ],
   "source": [
    "# L1 keys (items)\n",
    "l1_items = [x[0] for x in sorted_L1] \n",
    "\n",
    "print(f\"--- GENERATING C2 CANDIDATES ---\")\n",
    "print(f\"Items in L1: {len(l1_items)}\")\n",
    "print(f\"Valid Buckets: {valid_buckets}\")\n",
    "\n",
    "C2_candidates = []\n",
    "rejected_count = 0\n",
    "\n",
    "# Generate all pairs from L1 items\n",
    "possible_pairs = list(combinations(l1_items, 2))\n",
    "\n",
    "print(f\"Total possible pairs from L1: {len(possible_pairs)}\")\n",
    "print(\"-\" * 40)\n",
    "\n",
    "for pair in possible_pairs:\n",
    "    item1, item2 = pair\n",
    "    \n",
    "    # 1. Calculate Hash for this candidate\n",
    "    bucket = get_hash_bucket(item1, item2)\n",
    "    \n",
    "    # 2. DHP CHECK: Is this bucket in our valid list?\n",
    "    if bucket in valid_buckets:\n",
    "        C2_candidates.append(frozenset(pair))\n",
    "        # purely for display:\n",
    "        # print(f\"  Accepted: {pair} (Bucket {bucket})\") \n",
    "    else:\n",
    "        rejected_count += 1\n",
    "        print(f\"PRUNED by DHP: {pair} (Bucket {bucket})\")\n",
    "\n",
    "print(\"-\" * 40)\n",
    "print(f\"Candidates Generated (C2 Size): {len(C2_candidates)}\")\n",
    "print(f\"Candidates Pruned by DHP: {rejected_count}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "68d0f4ca",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-11-29T10:07:33.163774Z",
     "iopub.status.busy": "2025-11-29T10:07:33.163489Z",
     "iopub.status.idle": "2025-11-29T10:07:33.186122Z",
     "shell.execute_reply": "2025-11-29T10:07:33.184899Z"
    },
    "papermill": {
     "duration": 0.027819,
     "end_time": "2025-11-29T10:07:33.187535",
     "exception": false,
     "start_time": "2025-11-29T10:07:33.159716",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Step 1: Init Pass (Count L1 + Hash C2) ---\n",
      "L1 found: 7 items\n",
      "--- Step 2: Generate C2 (with DHP Pruning) ---\n",
      "C2 Candidates after DHP: 21 (out of 21 possible)\n",
      "--- Scanning DB for k=2 ---\n",
      "L2 found: 14 itemsets\n",
      "--- Scanning DB for k=3 ---\n",
      "L3 found: 12 itemsets\n",
      "--- Scanning DB for k=4 ---\n",
      "L4 found: 5 itemsets\n",
      "--- Scanning DB for k=5 ---\n",
      "L5 found: 1 itemsets\n",
      "\n",
      "==================================================\n",
      "FINAL OUTPUT\n",
      "==================================================\n",
      "Total Frequent Itemsets Found: 39\n",
      "Execution Time: 3.2921 ms\n",
      "Peak Memory Usage: 0.025233 MB\n",
      "--------------------------------------------------\n",
      "{Itemset: Support}\n",
      "{'Corn'}: 2\n",
      "{'Eggs'}: 4\n",
      "{'Kidney Beans'}: 5\n",
      "{'Milk'}: 3\n",
      "{'Nutmeg'}: 2\n",
      "{'Onion'}: 3\n",
      "{'Yogurt'}: 3\n",
      "{'Eggs', 'Kidney Beans'}: 4\n",
      "{'Milk', 'Eggs'}: 2\n",
      "{'Nutmeg', 'Eggs'}: 2\n",
      "{'Onion', 'Eggs'}: 3\n",
      "{'Eggs', 'Yogurt'}: 2\n",
      "{'Milk', 'Kidney Beans'}: 3\n",
      "{'Nutmeg', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Kidney Beans'}: 3\n",
      "{'Yogurt', 'Kidney Beans'}: 3\n",
      "{'Milk', 'Yogurt'}: 2\n",
      "{'Onion', 'Nutmeg'}: 2\n",
      "{'Nutmeg', 'Yogurt'}: 2\n",
      "{'Onion', 'Yogurt'}: 2\n",
      "{'Corn', 'Kidney Beans'}: 2\n",
      "{'Milk', 'Eggs', 'Kidney Beans'}: 2\n",
      "{'Nutmeg', 'Eggs', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Eggs', 'Kidney Beans'}: 3\n",
      "{'Eggs', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Nutmeg', 'Eggs'}: 2\n",
      "{'Nutmeg', 'Eggs', 'Yogurt'}: 2\n",
      "{'Onion', 'Eggs', 'Yogurt'}: 2\n",
      "{'Milk', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Nutmeg', 'Kidney Beans'}: 2\n",
      "{'Nutmeg', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Nutmeg', 'Yogurt'}: 2\n",
      "{'Onion', 'Nutmeg', 'Eggs', 'Kidney Beans'}: 2\n",
      "{'Nutmeg', 'Eggs', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Eggs', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Onion', 'Nutmeg', 'Eggs', 'Yogurt'}: 2\n",
      "{'Onion', 'Nutmeg', 'Yogurt', 'Kidney Beans'}: 2\n",
      "{'Nutmeg', 'Yogurt', 'Kidney Beans', 'Onion', 'Eggs'}: 2\n"
     ]
    }
   ],
   "source": [
    "def load_dataset():\n",
    "    return [\n",
    "        frozenset(['Milk', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt']),\n",
    "        frozenset(['Dill', 'Onion', 'Nutmeg', 'Kidney Beans', 'Eggs', 'Yogurt']),\n",
    "        frozenset(['Milk', 'Apple', 'Kidney Beans', 'Eggs']),\n",
    "        frozenset(['Milk', 'Unicorn', 'Corn', 'Kidney Beans', 'Yogurt']),\n",
    "        frozenset(['Corn', 'Onion', 'Onion', 'Kidney Beans', 'Ice cream', 'Eggs'])\n",
    "    ]\n",
    "\n",
    "def get_item_id(item_name):\n",
    "    \"\"\"Mapping items to integers for hashing.\"\"\"\n",
    "    mapping = {\n",
    "        'Apple': 1, 'Corn': 2, 'Dill': 3, 'Eggs': 4, \n",
    "        'Ice cream': 5, 'Kidney Beans': 6, 'Milk': 7, \n",
    "        'Nutmeg': 8, 'Onion': 9, 'Unicorn': 10, 'Yogurt': 11\n",
    "    }\n",
    "    return mapping.get(item_name, 0)\n",
    "\n",
    "def get_hash_bucket(item1, item2, bucket_count=7):\n",
    "    id1 = get_item_id(item1)\n",
    "    id2 = get_item_id(item2)\n",
    "    # Ensuring order doesn't change hash (commutative)\n",
    "    if id1 > id2: id1, id2 = id2, id1\n",
    "    return (id1 * 10 + id2) % bucket_count\n",
    "\n",
    "# --- 2. Core Functions ---\n",
    "\n",
    "def apriori_gen(Lk_minus_1, k):\n",
    "    \"\"\"\n",
    "    Generates Ck from L(k-1).\n",
    "    For k=2, we use simple combinations.\n",
    "    For k>2, we join sets that share k-2 items.\n",
    "    \"\"\"\n",
    "    candidates = []\n",
    "    len_Lk = len(Lk_minus_1)\n",
    "    lk_list = list(Lk_minus_1)\n",
    "    \n",
    "    if k == 2:\n",
    "        # Simple pairs\n",
    "        return list(combinations(lk_list, 2))\n",
    "    \n",
    "    # Standard Apriori Join for k > 2\n",
    "    for i in range(len_Lk):\n",
    "        for j in range(i + 1, len_Lk):\n",
    "            L1 = list(lk_list[i])\n",
    "            L2 = list(lk_list[j])\n",
    "            L1.sort(); L2.sort()\n",
    "            \n",
    "            # If first k-2 elements are equal, join them\n",
    "            if L1[:k-2] == L2[:k-2]:\n",
    "                candidates.append(lk_list[i] | lk_list[j])\n",
    "                \n",
    "    return candidates\n",
    "\n",
    "def scan_and_count(dataset, candidates):\n",
    "    \"\"\"Counts support for a list of candidates.\"\"\"\n",
    "    counts = defaultdict(int)\n",
    "    for tid in dataset:\n",
    "        for cand in candidates:\n",
    "            # cand can be a tuple (from combinations) or frozenset\n",
    "            cand_set = frozenset(cand)\n",
    "            if cand_set.issubset(tid):\n",
    "                counts[cand_set] += 1\n",
    "    return counts\n",
    "\n",
    "# --- 3. Main Algorithm with Metrics ---\n",
    "\n",
    "def run_apriori_dhp(min_support=2):\n",
    "    # Start Metrics\n",
    "    tracemalloc.start()\n",
    "    start_time = time.time()\n",
    "    \n",
    "    dataset = load_dataset()\n",
    "    bucket_count = 7\n",
    "    \n",
    "    global_frequent_itemsets = {} # Final Result Storage\n",
    "    \n",
    "    print(\"--- Step 1: Init Pass (Count L1 + Hash C2) ---\")\n",
    "    c1_counts = defaultdict(int)\n",
    "    bucket_counts = defaultdict(int)\n",
    "    \n",
    "    for transaction in dataset:\n",
    "        # Count Items\n",
    "        for item in transaction:\n",
    "            c1_counts[item] += 1\n",
    "            \n",
    "        # Hash Pairs (DHP)\n",
    "        items = list(transaction)\n",
    "        for i in range(len(items)):\n",
    "            for j in range(i + 1, len(items)):\n",
    "                b_idx = get_hash_bucket(items[i], items[j], bucket_count)\n",
    "                bucket_counts[b_idx] += 1\n",
    "\n",
    "    # Generate L1\n",
    "    L1 = [item for item, count in c1_counts.items() if count >= min_support]\n",
    "    L1.sort() # Sorting helps deterministic behavior\n",
    "    \n",
    "    # Store L1\n",
    "    for item in L1:\n",
    "        global_frequent_itemsets[frozenset([item])] = c1_counts[item]\n",
    "\n",
    "    print(f\"L1 found: {len(L1)} items\")\n",
    "    \n",
    "    # --- Step 2: Generate C2 using DHP Filter ---\n",
    "    print(\"--- Step 2: Generate C2 (with DHP Pruning) ---\")\n",
    "    valid_buckets = {b for b, cnt in bucket_counts.items() if cnt >= min_support}\n",
    "    \n",
    "    C2_candidates = []\n",
    "    l1_pairs = list(combinations(L1, 2))\n",
    "    \n",
    "    for pair in l1_pairs:\n",
    "        b_idx = get_hash_bucket(pair[0], pair[1], bucket_count)\n",
    "        if b_idx in valid_buckets:\n",
    "            C2_candidates.append(frozenset(pair))\n",
    "    \n",
    "    print(f\"C2 Candidates after DHP: {len(C2_candidates)} (out of {len(l1_pairs)} possible)\")\n",
    "    \n",
    "    # Loop Variables\n",
    "    current_C = C2_candidates\n",
    "    k = 2\n",
    "    \n",
    "    # --- Step 3: Iterative Loop (L2, L3...) ---\n",
    "    while len(current_C) > 0:\n",
    "        print(f\"--- Scanning DB for k={k} ---\")\n",
    "        \n",
    "        # Count Support\n",
    "        candidates_counts = scan_and_count(dataset, current_C)\n",
    "        \n",
    "        # Filter L_k\n",
    "        L_k = []\n",
    "        for cand, count in candidates_counts.items():\n",
    "            if count >= min_support:\n",
    "                L_k.append(cand)\n",
    "                global_frequent_itemsets[cand] = count\n",
    "        \n",
    "        print(f\"L{k} found: {len(L_k)} itemsets\")\n",
    "        \n",
    "        if len(L_k) <= 1:\n",
    "            break # Cannot generate C_(k+1) from 1 or 0 items\n",
    "            \n",
    "        # Generate Next Candidates (C_k+1) - Standard Join\n",
    "        k += 1\n",
    "        current_C = apriori_gen(L_k, k)\n",
    "        \n",
    "    # End Metrics\n",
    "    end_time = time.time()\n",
    "    current_mem, peak_mem = tracemalloc.get_traced_memory()\n",
    "    tracemalloc.stop()\n",
    "    \n",
    "    execution_time = (end_time - start_time) * 1000 # ms\n",
    "    peak_mem_mb = peak_mem / (1024 * 1024)\n",
    "    \n",
    "    return global_frequent_itemsets, execution_time, peak_mem_mb\n",
    "\n",
    "# --- 4. Execution ---\n",
    "\n",
    "final_results, exec_time, peak_memory = run_apriori_dhp(min_support=2)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL OUTPUT\")\n",
    "print(\"=\"*50)\n",
    "print(f\"Total Frequent Itemsets Found: {len(final_results)}\")\n",
    "print(f\"Execution Time: {exec_time:.4f} ms\")\n",
    "print(f\"Peak Memory Usage: {peak_memory:.6f} MB\")\n",
    "print(\"-\" * 50)\n",
    "print(\"{Itemset: Support}\")\n",
    "for itemset, support in final_results.items():\n",
    "    # formatting frozenset to look cleaner\n",
    "    clean_set = set(itemset) \n",
    "    print(f\"{clean_set}: {support}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "dockerImageVersionId": 31192,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 7.694434,
   "end_time": "2025-11-29T10:07:33.708873",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-11-29T10:07:26.014439",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
