{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13920997,"sourceType":"datasetVersion","datasetId":8870732}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-29T15:55:44.223511Z","iopub.execute_input":"2025-11-29T15:55:44.223764Z","iopub.status.idle":"2025-11-29T15:55:46.015486Z","shell.execute_reply.started":"2025-11-29T15:55:44.223731Z","shell.execute_reply":"2025-11-29T15:55:46.014578Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/associative-rules-data/retail_transactional.csv\n/kaggle/input/associative-rules-data/groceries.csv\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport csv\nimport time\nimport tracemalloc\nfrom itertools import combinations","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:13:33.288949Z","iopub.execute_input":"2025-11-29T17:13:33.289377Z","iopub.status.idle":"2025-11-29T17:13:33.300370Z","shell.execute_reply.started":"2025-11-29T17:13:33.289344Z","shell.execute_reply":"2025-11-29T17:13:33.299386Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\n# File paths\nGROCERIES_PATH = '/kaggle/input/associative-rules-data/groceries.csv'\nRETAIL_PATH = '/kaggle/input/associative-rules-data/retail_transactional.csv'\n\ndef inspect_and_analyze(file_path, name):\n    print(f\"\\n{'='*50}\")\n    print(f\" Analyzing Dataset: {name}\")\n    print(f\"{'='*50}\")\n    \n    if not os.path.exists(file_path):\n        print(f\" File not found: {file_path}\")\n        return\n\n    # 1. Preview raw content (First 5 lines) to understand structure\n    print(\" Raw File Content (First 5 lines):\")\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            raw_lines = [f.readline().strip() for _ in range(5)]\n            for i, line in enumerate(raw_lines):\n                print(f\"  Line {i+1}: {line}\")\n    except Exception as e:\n        print(f\"  Could not read raw lines: {e}\")\n        return\n\n    # 2. Process file line by line to calculate statistics\n    transactions = []\n    unique_items = set()\n    \n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                \n                # Split by comma (assuming standard CSV or basket format)\n                # We remove quotes to clean up data like \"milk\" -> milk\n                items = [i.strip().replace('\"', '') for i in line.split(',')]\n                \n                # Filter out empty strings\n                items = [i for i in items if i]\n                \n                if items:\n                    transactions.append(items)\n                    unique_items.update(items)\n\n        # 3. Calculate Statistics\n        num_transactions = len(transactions)\n        num_unique_items = len(unique_items)\n        \n        if num_transactions == 0:\n            print(\" Dataset appears empty.\")\n            return\n\n        avg_len = sum(len(t) for t in transactions) / num_transactions\n        \n        # Calculate Top 5 Frequent Items\n        all_items = [item for t in transactions for item in t]\n        item_counts = Counter(all_items)\n        top_5 = item_counts.most_common(5)\n\n        print(f\"\\n Statistics Summary:\")\n        print(f\"  • Total Transactions (Rows): {num_transactions:,}\")\n        print(f\"  • Unique Items Found: {num_unique_items:,}\")\n        print(f\"  • Avg Items per Row: {avg_len:.2f}\")\n        \n        print(f\"\\n Top 5 Frequent Items (Check for headers or strange IDs):\")\n        for item, count in top_5:\n            support = count / num_transactions\n            print(f\"  - '{item}': {count} times (Freq: {support:.4f})\")\n\n    except Excep as e:\n        print(f\" Error calculating stats: {e}\")\n\n# Run analysis\ninspect_and_analyze(GROCERIES_PATH, \"Groceries\")\ninspect_and_analyze(RETAIL_PATH, \"Retail Transactional\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T16:50:03.427699Z","iopub.execute_input":"2025-11-29T16:50:03.428071Z","iopub.status.idle":"2025-11-29T16:50:04.439824Z","shell.execute_reply.started":"2025-11-29T16:50:03.428041Z","shell.execute_reply":"2025-11-29T16:50:04.438676Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n Analyzing Dataset: Groceries\n==================================================\n Raw File Content (First 5 lines):\n  Line 1: citrus fruit,semi-finished bread,margarine,ready soups\n  Line 2: tropical fruit,yogurt,coffee\n  Line 3: whole milk\n  Line 4: pip fruit,yogurt,cream cheese ,meat spreads\n  Line 5: other vegetables,whole milk,condensed milk,long life bakery product\n\n Statistics Summary:\n  • Total Transactions (Rows): 9,835\n  • Unique Items Found: 169\n  • Avg Items per Row: 4.41\n\n Top 5 Frequent Items (Check for headers or strange IDs):\n  - 'whole milk': 2513 times (Freq: 0.2555)\n  - 'other vegetables': 1903 times (Freq: 0.1935)\n  - 'rolls/buns': 1809 times (Freq: 0.1839)\n  - 'soda': 1715 times (Freq: 0.1744)\n  - 'yogurt': 1372 times (Freq: 0.1395)\n\n==================================================\n Analyzing Dataset: Retail Transactional\n==================================================\n Raw File Content (First 5 lines):\n  Line 1: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29\n  Line 2: 30,31,32\n  Line 3: 33,34,35\n  Line 4: 36,37,38,39,40,41,42,43,44,45,46\n  Line 5: 38,39,47,48\n\n Statistics Summary:\n  • Total Transactions (Rows): 88,162\n  • Unique Items Found: 16,470\n  • Avg Items per Row: 10.31\n\n Top 5 Frequent Items (Check for headers or strange IDs):\n  - '39': 50675 times (Freq: 0.5748)\n  - '48': 42135 times (Freq: 0.4779)\n  - '38': 15596 times (Freq: 0.1769)\n  - '32': 15167 times (Freq: 0.1720)\n  - '41': 14945 times (Freq: 0.1695)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"markdown","source":"# Analysis of Groceries Dataset (The “Control” Group)\n- Structure: It is a Sparse Dataset.\n- Volume: ~10k transactions.\n- Dimensionality: Only 169 unique items.\n- Implication for Algorithms:\n- Standard Apriori: Will run very fast. Generating pairs from 169 items is trivial for modern CPUs.\n- DHP:  probably won’t see a huge performance gain here because the overhead of hashing might outweigh the benefit of pruning such a small candidate set.\n- Sampling: With 10k rows, a 10% sample is only ~980 rows. This is small enough that statistical variance might cause some False Negatives at low support thresholds.\n\n# Analysis of Retail Transactional Dataset (The “Stress Test”)\n- Structure: It is a High-Dimensional / Dense Core Dataset.\n- Volume: ~88k transactions (9x larger than Groceries).\n- Dimensionality: 16,470 Unique Items!\n- The “Explosion” Risk:\n- In Standard Apriori, step k= 2  tries to generate pairs from frequent single items.\n- Worst case scenario (if many items are frequent):135,630,465 pairs.\n- Item Distribution: Item '39' appears in 57% of baskets. Item '48' in 47%. This means L1 (Frequent 1-itemsets) will be large, guaranteeing that C2 will be huge.\n","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# 1. DATA LOADING UTILS\n# ==========================================\ndef load_dataset(file_path):\n    \n    dataset = []\n    print(f\" Loading: {file_path} ...\")\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                # Clean and split items\n                items = [i.strip().replace('\"', '') for i in line.split(',')]\n                items = [i for i in items if i] # Remove empty strings\n                if items:\n                    dataset.append(frozenset(items))\n        print(f\"✅ Loaded {len(dataset)} transactions.\")\n        return dataset\n    except Exception as e:\n        print(f\" Error loading file: {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:13:54.397925Z","iopub.execute_input":"2025-11-29T17:13:54.398308Z","iopub.status.idle":"2025-11-29T17:13:54.405551Z","shell.execute_reply.started":"2025-11-29T17:13:54.398280Z","shell.execute_reply":"2025-11-29T17:13:54.404306Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"# ==========================================\n# 2. HELPER FUNCTIONS (The Engine)\n# ==========================================\n\ndef create_c1(dataset):\n    c1 = {}\n    for transaction in dataset:\n        for item in transaction:\n            itemset = frozenset([item])\n            c1[itemset] = c1.get(itemset, 0) + 1\n    return c1\n\ndef filter_candidates(candidates_counts, min_support_count):\n    ret_l = {}\n    for key, value in candidates_counts.items():\n        if value >= min_support_count:\n            ret_l[key] = value\n    return ret_l\n\ndef generate_Ck(Lk_minus_1, k):\n   \n    candidates = set()\n    lk_list = list(Lk_minus_1.keys())\n    len_lk = len(lk_list)\n    \n    # Join Step\n    for i in range(len_lk):\n        for j in range(i + 1, len_lk):\n            l1 = list(lk_list[i])\n            l2 = list(lk_list[j])\n            l1.sort()\n            l2.sort()\n            \n            # Check if first k-2 items are equal\n            if l1[:k-2] == l2[:k-2]:\n                # Union creates the new candidate\n                candidate = lk_list[i] | lk_list[j]\n                \n                # Prune Step: Check if all subsets are frequent\n                # (Optional for speed in Python, but standard for strict Apriori)\n                if has_infrequent_subset(candidate, Lk_minus_1):\n                    continue\n                candidates.add(candidate)\n    return candidates\n\ndef has_infrequent_subset(candidate, Lk_minus_1):\n    # generate all subsets of size k-1\n    for subset in combinations(candidate, len(candidate) - 1):\n        if frozenset(subset) not in Lk_minus_1:\n            return True\n    return False\n\ndef count_candidates(dataset, candidates):\n    counts = {cand: 0 for cand in candidates}\n    \n    # Optimization: Iterate through dataset once\n    for transaction in dataset:\n        # For each candidate, check if it is in transaction\n        # Optimization for k=2: Double loop over transaction items is sometimes faster than subset check\n        # But for general k, issubset is safest in Python\n        for cand in candidates:\n            if cand.issubset(transaction):\n                counts[cand] += 1\n    return counts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:14:24.336557Z","iopub.execute_input":"2025-11-29T17:14:24.336949Z","iopub.status.idle":"2025-11-29T17:14:24.349302Z","shell.execute_reply.started":"2025-11-29T17:14:24.336917Z","shell.execute_reply":"2025-11-29T17:14:24.348111Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# ==========================================\n# 3. STANDARD APRIORI ALGORITHM (Instrumented)\n# ==========================================\n\ndef run_standard_apriori(dataset, min_support_fraction):\n    \n    tracemalloc.start()\n    start_time = time.time()\n    \n    min_support_count = int(len(dataset) * min_support_fraction)\n    stats = {\n        \"algorithm\": \"Standard Apriori\",\n        \"min_sup\": min_support_fraction,\n        \"c2_count\": 0, # To store |C2|\n        \"total_freq_itemsets\": 0,\n        \"execution_time\": 0,\n        \"memory_peak_mb\": 0\n    }\n    \n    L_global = {}\n    \n    # --- Step 1: C1 & L1 ---\n    print(f\"   [Standard] Generating L1 (Min Count: {min_support_count})...\")\n    C1_counts = create_c1(dataset)\n    L1 = filter_candidates(C1_counts, min_support_count)\n    L_global.update(L1)\n    \n    L_current = L1\n    k = 2\n    \n    while L_current:\n        # print(f\"   [Standard] Step k={k}...\")\n        \n        # Generate Candidates\n        Ck = generate_Ck(L_current, k)\n        \n        # *** CAPTURE |C2| METRIC ***\n        if k == 2:\n            stats[\"c2_count\"] = len(Ck)\n            print(f\"   [Metric Captured] |C2_standard| = {len(Ck)}\")\n        \n        if not Ck:\n            break\n            \n        # Count Candidates\n        Ck_counts = count_candidates(dataset, Ck)\n        \n        # Filter\n        Lk = filter_candidates(Ck_counts, min_support_count)\n        \n        if not Lk:\n            break\n            \n        L_global.update(Lk)\n        L_current = Lk\n        k += 1\n\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    \n    stats[\"total_freq_itemsets\"] = len(L_global)\n    stats[\"execution_time\"] = end_time - start_time\n    stats[\"memory_peak_mb\"] = peak / (1024 * 1024)\n    \n    return L_global, stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:15:35.914372Z","iopub.execute_input":"2025-11-29T17:15:35.914774Z","iopub.status.idle":"2025-11-29T17:15:35.925267Z","shell.execute_reply.started":"2025-11-29T17:15:35.914744Z","shell.execute_reply":"2025-11-29T17:15:35.923650Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"\nGROCERIES_PATH = '/kaggle/input/associative-rules-data/groceries.csv'\n\ngroceries_data = load_dataset(GROCERIES_PATH)\n\nmin_sup_list = [0.05, 0.03, 0.02, 0.01]\n\nresults_summary = []\n\nif groceries_data:\n    print(f\"\\n{'='*60}\")\n    print(f\" STARTING STANDARD APRIORI TEST LOOP (Groceries)\")\n    print(f\"{'='*60}\")\n\n    for min_sup in min_sup_list:\n        print(f\"\\nTesting Min Support: {min_sup}\")\n        \n        _, metrics = run_standard_apriori(groceries_data, min_support_fraction=min_sup)\n        \n        results_summary.append(metrics)\n        \n        print(f\"    Done.\")\n        print(f\"   • |C2_standard|:    {metrics['c2_count']}\")\n        print(f\"   • Frequent Sets:    {metrics['total_freq_itemsets']}\")\n        print(f\"   • Execution Time:   {metrics['execution_time']:.4f} s\")\n        print(f\"   • Peak Memory:      {metrics['memory_peak_mb']:.4f} MB\")\n\n   \n    print(f\"\\n{'='*60}\")\n    print(\" FINAL SUMMARY TABLE (Standard Apriori)\")\n    print(f\"{'='*60}\")\n    \n    df_results = pd.DataFrame(results_summary)\n    cols = ['min_sup', 'c2_count', 'total_freq_itemsets', 'execution_time', 'memory_peak_mb']\n    print(df_results[cols].to_string(index=False))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-29T17:25:17.196174Z","iopub.execute_input":"2025-11-29T17:25:17.196592Z","iopub.status.idle":"2025-11-29T17:25:23.833583Z","shell.execute_reply.started":"2025-11-29T17:25:17.196562Z","shell.execute_reply":"2025-11-29T17:25:23.832747Z"}},"outputs":[{"name":"stdout","text":" Loading: /kaggle/input/associative-rules-data/groceries.csv ...\n✅ Loaded 9835 transactions.\n\n============================================================\n STARTING STANDARD APRIORI TEST LOOP (Groceries)\n============================================================\n\nTesting Min Support: 0.05\n   [Standard] Generating L1 (Min Count: 491)...\n   [Metric Captured] |C2_standard| = 378\n    Done.\n   • |C2_standard|:    378\n   • Frequent Sets:    31\n   • Execution Time:   0.4329 s\n   • Peak Memory:      0.1830 MB\n\nTesting Min Support: 0.03\n   [Standard] Generating L1 (Min Count: 295)...\n   [Metric Captured] |C2_standard| = 946\n    Done.\n   • |C2_standard|:    946\n   • Frequent Sets:    64\n   • Execution Time:   0.8367 s\n   • Peak Memory:      0.3252 MB\n\nTesting Min Support: 0.02\n   [Standard] Generating L1 (Min Count: 196)...\n   [Metric Captured] |C2_standard| = 1711\n    Done.\n   • |C2_standard|:    1711\n   • Frequent Sets:    123\n   • Execution Time:   1.5250 s\n   • Peak Memory:      0.6292 MB\n\nTesting Min Support: 0.01\n   [Standard] Generating L1 (Min Count: 98)...\n   [Metric Captured] |C2_standard| = 3828\n    Done.\n   • |C2_standard|:    3828\n   • Frequent Sets:    341\n   • Execution Time:   3.7930 s\n   • Peak Memory:      1.2775 MB\n\n============================================================\n FINAL SUMMARY TABLE (Standard Apriori)\n============================================================\n min_sup  c2_count  total_freq_itemsets  execution_time  memory_peak_mb\n    0.05       378                   31        0.432902        0.182958\n    0.03       946                   64        0.836691        0.325210\n    0.02      1711                  123        1.524990        0.629250\n    0.01      3828                  341        3.792999        1.277456\n","output_type":"stream"}],"execution_count":17}]}