{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":13920997,"sourceType":"datasetVersion","datasetId":8870732}],"dockerImageVersionId":31192,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:15.554038Z","iopub.execute_input":"2025-11-30T06:59:15.554893Z","iopub.status.idle":"2025-11-30T06:59:15.937563Z","shell.execute_reply.started":"2025-11-30T06:59:15.554857Z","shell.execute_reply":"2025-11-30T06:59:15.936560Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/associative-rules-data/retail_transactional.csv\n/kaggle/input/associative-rules-data/groceries.csv\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\nimport csv\nimport time\nimport tracemalloc\nfrom itertools import combinations\nfrom collections import defaultdict\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:07:34.060005Z","iopub.execute_input":"2025-11-30T07:07:34.060614Z","iopub.status.idle":"2025-11-30T07:07:34.077105Z","shell.execute_reply.started":"2025-11-30T07:07:34.060581Z","shell.execute_reply":"2025-11-30T07:07:34.074752Z"}},"outputs":[],"execution_count":12},{"cell_type":"code","source":"import os\nfrom collections import Counter\n\n# File paths\nGROCERIES_PATH = '/kaggle/input/associative-rules-data/groceries.csv'\nRETAIL_PATH = '/kaggle/input/associative-rules-data/retail_transactional.csv'\n\ndef inspect_and_analyze(file_path, name):\n    print(f\"\\n{'='*50}\")\n    print(f\" Analyzing Dataset: {name}\")\n    print(f\"{'='*50}\")\n    \n    if not os.path.exists(file_path):\n        print(f\" File not found: {file_path}\")\n        return\n\n    # 1. Preview raw content (First 5 lines) to understand structure\n    print(\" Raw File Content (First 5 lines):\")\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            raw_lines = [f.readline().strip() for _ in range(5)]\n            for i, line in enumerate(raw_lines):\n                print(f\"  Line {i+1}: {line}\")\n    except Exception as e:\n        print(f\"  Could not read raw lines: {e}\")\n        return\n\n    # 2. Process file line by line to calculate statistics\n    transactions = []\n    unique_items = set()\n    \n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                \n                # Split by comma (assuming standard CSV or basket format)\n                # We remove quotes to clean up data like \"milk\" -> milk\n                items = [i.strip().replace('\"', '') for i in line.split(',')]\n                \n                # Filter out empty strings\n                items = [i for i in items if i]\n                \n                if items:\n                    transactions.append(items)\n                    unique_items.update(items)\n\n        # 3. Calculate Statistics\n        num_transactions = len(transactions)\n        num_unique_items = len(unique_items)\n        \n        if num_transactions == 0:\n            print(\" Dataset appears empty.\")\n            return\n\n        avg_len = sum(len(t) for t in transactions) / num_transactions\n        \n        # Calculate Top 5 Frequent Items\n        all_items = [item for t in transactions for item in t]\n        item_counts = Counter(all_items)\n        top_5 = item_counts.most_common(5)\n\n        print(f\"\\n Statistics Summary:\")\n        print(f\"  â€¢ Total Transactions (Rows): {num_transactions:,}\")\n        print(f\"  â€¢ Unique Items Found: {num_unique_items:,}\")\n        print(f\"  â€¢ Avg Items per Row: {avg_len:.2f}\")\n        \n        print(f\"\\n Top 5 Frequent Items (Check for headers or strange IDs):\")\n        for item, count in top_5:\n            support = count / num_transactions\n            print(f\"  - '{item}': {count} times (Freq: {support:.4f})\")\n\n    except Excep as e:\n        print(f\" Error calculating stats: {e}\")\n\n# Run analysis\ninspect_and_analyze(GROCERIES_PATH, \"Groceries\")\ninspect_and_analyze(RETAIL_PATH, \"Retail Transactional\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:17.417749Z","iopub.execute_input":"2025-11-30T06:59:17.418293Z","iopub.status.idle":"2025-11-30T06:59:18.256485Z","shell.execute_reply.started":"2025-11-30T06:59:17.418257Z","shell.execute_reply":"2025-11-30T06:59:18.255408Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\n Analyzing Dataset: Groceries\n==================================================\n Raw File Content (First 5 lines):\n  Line 1: citrus fruit,semi-finished bread,margarine,ready soups\n  Line 2: tropical fruit,yogurt,coffee\n  Line 3: whole milk\n  Line 4: pip fruit,yogurt,cream cheese ,meat spreads\n  Line 5: other vegetables,whole milk,condensed milk,long life bakery product\n\n Statistics Summary:\n  â€¢ Total Transactions (Rows): 9,835\n  â€¢ Unique Items Found: 169\n  â€¢ Avg Items per Row: 4.41\n\n Top 5 Frequent Items (Check for headers or strange IDs):\n  - 'whole milk': 2513 times (Freq: 0.2555)\n  - 'other vegetables': 1903 times (Freq: 0.1935)\n  - 'rolls/buns': 1809 times (Freq: 0.1839)\n  - 'soda': 1715 times (Freq: 0.1744)\n  - 'yogurt': 1372 times (Freq: 0.1395)\n\n==================================================\n Analyzing Dataset: Retail Transactional\n==================================================\n Raw File Content (First 5 lines):\n  Line 1: 0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24,25,26,27,28,29\n  Line 2: 30,31,32\n  Line 3: 33,34,35\n  Line 4: 36,37,38,39,40,41,42,43,44,45,46\n  Line 5: 38,39,47,48\n\n Statistics Summary:\n  â€¢ Total Transactions (Rows): 88,162\n  â€¢ Unique Items Found: 16,470\n  â€¢ Avg Items per Row: 10.31\n\n Top 5 Frequent Items (Check for headers or strange IDs):\n  - '39': 50675 times (Freq: 0.5748)\n  - '48': 42135 times (Freq: 0.4779)\n  - '38': 15596 times (Freq: 0.1769)\n  - '32': 15167 times (Freq: 0.1720)\n  - '41': 14945 times (Freq: 0.1695)\n","output_type":"stream"}],"execution_count":4},{"cell_type":"markdown","source":"# Analysis of Groceries Dataset (The â€œControlâ€ Group)\n- Structure: It is a Sparse Dataset.\n- Volume: ~10k transactions.\n- Dimensionality: Only 169 unique items.\n- Implication for Algorithms:\n- Standard Apriori: Will run very fast. Generating pairs from 169 items is trivial for modern CPUs.\n- DHP:  probably wonâ€™t see a huge performance gain here because the overhead of hashing might outweigh the benefit of pruning such a small candidate set.\n- Sampling: With 10k rows, a 10% sample is only ~980 rows. This is small enough that statistical variance might cause some False Negatives at low support thresholds.\n\n# Analysis of Retail Transactional Dataset (The â€œStress Testâ€)\n- Structure: It is a High-Dimensional / Dense Core Dataset.\n- Volume: ~88k transactions (9x larger than Groceries).\n- Dimensionality: 16,470 Unique Items!\n- The â€œExplosionâ€ Risk:\n- In Standard Apriori, step k= 2  tries to generate pairs from frequent single items.\n- Worst case scenario (if many items are frequent):135,630,465 pairs.\n- Item Distribution: Item '39' appears in 57% of baskets. Item '48' in 47%. This means L1 (Frequent 1-itemsets) will be large, guaranteeing that C2 will be huge.\n","metadata":{}},{"cell_type":"code","source":"# ==========================================\n# 1. DATA LOADING UTILS\n# ==========================================\ndef load_dataset(file_path):\n    \n    dataset = []\n    print(f\" Loading: {file_path} ...\")\n    try:\n        with open(file_path, 'r', encoding='utf-8', errors='replace') as f:\n            for line in f:\n                line = line.strip()\n                if not line: continue\n                # Clean and split items\n                items = [i.strip().replace('\"', '') for i in line.split(',')]\n                items = [i for i in items if i] # Remove empty strings\n                if items:\n                    dataset.append(frozenset(items))\n        print(f\" Loaded {len(dataset)} transactions.\")\n        return dataset\n    except Exception as e:\n        print(f\" Error loading file: {e}\")\n        return []","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:18.258788Z","iopub.execute_input":"2025-11-30T06:59:18.259077Z","iopub.status.idle":"2025-11-30T06:59:18.265862Z","shell.execute_reply.started":"2025-11-30T06:59:18.259054Z","shell.execute_reply":"2025-11-30T06:59:18.264593Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ==========================================\n# 2. HELPER FUNCTIONS (The Engine)\n# ==========================================\n\ndef create_c1(dataset):\n    c1 = {}\n    for transaction in dataset:\n        for item in transaction:\n            itemset = frozenset([item])\n            c1[itemset] = c1.get(itemset, 0) + 1\n    return c1\n\ndef filter_candidates(candidates_counts, min_support_count):\n    ret_l = {}\n    for key, value in candidates_counts.items():\n        if value >= min_support_count:\n            ret_l[key] = value\n    return ret_l\n\ndef generate_Ck(Lk_minus_1, k):\n   \n    candidates = set()\n    lk_list = list(Lk_minus_1.keys())\n    len_lk = len(lk_list)\n    \n    # Join Step\n    for i in range(len_lk):\n        for j in range(i + 1, len_lk):\n            l1 = list(lk_list[i])\n            l2 = list(lk_list[j])\n            l1.sort()\n            l2.sort()\n            \n            # Check if first k-2 items are equal\n            if l1[:k-2] == l2[:k-2]:\n                # Union creates the new candidate\n                candidate = lk_list[i] | lk_list[j]\n                \n                # Prune Step: Check if all subsets are frequent\n                # (Optional for speed in Python, but standard for strict Apriori)\n                if has_infrequent_subset(candidate, Lk_minus_1):\n                    continue\n                candidates.add(candidate)\n    return candidates\n\ndef has_infrequent_subset(candidate, Lk_minus_1):\n    # generate all subsets of size k-1\n    for subset in combinations(candidate, len(candidate) - 1):\n        if frozenset(subset) not in Lk_minus_1:\n            return True\n    return False\n\ndef count_candidates(dataset, candidates):\n    counts = {cand: 0 for cand in candidates}\n    \n    # Optimization: Iterate through dataset once\n    for transaction in dataset:\n        # For each candidate, check if it is in transaction\n        # Optimization for k=2: Double loop over transaction items is sometimes faster than subset check\n        # But for general k, issubset is safest in Python\n        for cand in candidates:\n            if cand.issubset(transaction):\n                counts[cand] += 1\n    return counts\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:18.266698Z","iopub.execute_input":"2025-11-30T06:59:18.267633Z","iopub.status.idle":"2025-11-30T06:59:18.289390Z","shell.execute_reply.started":"2025-11-30T06:59:18.267598Z","shell.execute_reply":"2025-11-30T06:59:18.288360Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"# ==========================================\n# 3. STANDARD APRIORI ALGORITHM (Instrumented)\n# ==========================================\n\ndef run_standard_apriori(dataset, min_support_fraction):\n    \n    tracemalloc.start()\n    start_time = time.time()\n    \n    min_support_count = int(len(dataset) * min_support_fraction)\n    stats = {\n        \"algorithm\": \"Standard Apriori\",\n        \"min_sup\": min_support_fraction,\n        \"c2_count\": 0, # To store |C2|\n        \"total_freq_itemsets\": 0,\n        \"execution_time\": 0,\n        \"memory_peak_mb\": 0\n    }\n    \n    L_global = {}\n    \n    # --- Step 1: C1 & L1 ---\n    print(f\"   [Standard] Generating L1 (Min Count: {min_support_count})...\")\n    C1_counts = create_c1(dataset)\n    L1 = filter_candidates(C1_counts, min_support_count)\n    L_global.update(L1)\n    \n    L_current = L1\n    k = 2\n    \n    while L_current:\n        # print(f\"   [Standard] Step k={k}...\")\n        \n        # Generate Candidates\n        Ck = generate_Ck(L_current, k)\n        \n        # *** CAPTURE |C2| METRIC ***\n        if k == 2:\n            stats[\"c2_count\"] = len(Ck)\n            print(f\"   [Metric Captured] |C2_standard| = {len(Ck)}\")\n        \n        if not Ck:\n            break\n            \n        # Count Candidates\n        Ck_counts = count_candidates(dataset, Ck)\n        \n        # Filter\n        Lk = filter_candidates(Ck_counts, min_support_count)\n        \n        if not Lk:\n            break\n            \n        L_global.update(Lk)\n        L_current = Lk\n        k += 1\n\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    \n    stats[\"total_freq_itemsets\"] = len(L_global)\n    stats[\"execution_time\"] = end_time - start_time\n    stats[\"memory_peak_mb\"] = peak / (1024 * 1024)\n    \n    return L_global, stats","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:18.290418Z","iopub.execute_input":"2025-11-30T06:59:18.290705Z","iopub.status.idle":"2025-11-30T06:59:18.311654Z","shell.execute_reply.started":"2025-11-30T06:59:18.290683Z","shell.execute_reply":"2025-11-30T06:59:18.310452Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"\n\ndatasets_config = [\n    {\n        'name': 'GROCERIES',\n        'path': '/kaggle/input/associative-rules-data/groceries.csv'\n    },\n    {\n        'name': 'RETAIL',\n        'path': '/kaggle/input/associative-rules-data/retail_transactional.csv' \n    }\n]\n\n# The list of support values to test\nmin_sup_list = [0.05, 0.03, 0.02, 0.01]\n\n# Container for all results\nall_results = []\n\n# ==========================================\n# EXECUTION LOOP\n# ==========================================\n\nfor ds in datasets_config:\n    print(f\"\\n{'#'*60}\")\n    print(f\" PROCESSING DATASET: {ds['name']}\")\n    print(f\"{'#'*60}\")\n    \n    # 1. Load the dataset\n    dataset = load_dataset(ds['path'])\n    \n    if not dataset:\n        print(f\" Failed to load {ds['name']}. Skipping...\")\n        continue\n\n    # 2. Run the loop for each min_sup\n    print(f\"   Dataset size: {len(dataset)} transactions.\")\n    \n    for min_sup in min_sup_list:\n        print(f\"\\n    Testing Min Support: {min_sup}\")\n        \n        try:\n            # Run Standard Apriori\n            _, metrics = run_standard_apriori(dataset, min_support_fraction=min_sup)\n            \n            # Tag the metrics with the dataset name for the final table\n            metrics['dataset'] = ds['name']\n            all_results.append(metrics)\n            \n            print(f\"       Done.\")\n            print(f\"      â€¢ |C2_standard|:    {metrics['c2_count']}\")\n            print(f\"      â€¢ Frequent Sets:    {metrics['total_freq_itemsets']}\")\n            print(f\"      â€¢ Execution Time:   {metrics['execution_time']:.4f} s\")\n            print(f\"      â€¢ Peak Memory:      {metrics['memory_peak_mb']:.4f} MB\")\n            \n        except Exception as e:\n            print(f\"       Error running {ds['name']} at {min_sup}: {e}\")\n\n# ==========================================\n# FINAL COMPARISON TABLE\n# ==========================================\nprint(f\"\\n{'='*60}\")\nprint(\" FINAL COMBINED SUMMARY (Standard Apriori)\")\nprint(f\"{'='*60}\")\n\nif all_results:\n    df_results = pd.DataFrame(all_results)\n    # Reorder columns: Dataset first, then the metrics\n    cols = ['dataset', 'min_sup', 'c2_count', 'total_freq_itemsets', 'execution_time', 'memory_peak_mb']\n    print(df_results[cols].to_string(index=False))\nelse:\n    print(\"No results to display.\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:18.312784Z","iopub.execute_input":"2025-11-30T06:59:18.313095Z","iopub.status.idle":"2025-11-30T06:59:57.179517Z","shell.execute_reply.started":"2025-11-30T06:59:18.313068Z","shell.execute_reply":"2025-11-30T06:59:57.178690Z"}},"outputs":[{"name":"stdout","text":"\n############################################################\n PROCESSING DATASET: GROCERIES\n############################################################\n Loading: /kaggle/input/associative-rules-data/groceries.csv ...\n Loaded 9835 transactions.\n   Dataset size: 9835 transactions.\n\n    Testing Min Support: 0.05\n   [Standard] Generating L1 (Min Count: 491)...\n   [Metric Captured] |C2_standard| = 378\n       Done.\n      â€¢ |C2_standard|:    378\n      â€¢ Frequent Sets:    31\n      â€¢ Execution Time:   0.4253 s\n      â€¢ Peak Memory:      0.1853 MB\n\n    Testing Min Support: 0.03\n   [Standard] Generating L1 (Min Count: 295)...\n   [Metric Captured] |C2_standard| = 946\n       Done.\n      â€¢ |C2_standard|:    946\n      â€¢ Frequent Sets:    64\n      â€¢ Execution Time:   0.8476 s\n      â€¢ Peak Memory:      0.3253 MB\n\n    Testing Min Support: 0.02\n   [Standard] Generating L1 (Min Count: 196)...\n   [Metric Captured] |C2_standard| = 1711\n       Done.\n      â€¢ |C2_standard|:    1711\n      â€¢ Frequent Sets:    123\n      â€¢ Execution Time:   1.5347 s\n      â€¢ Peak Memory:      0.6293 MB\n\n    Testing Min Support: 0.01\n   [Standard] Generating L1 (Min Count: 98)...\n   [Metric Captured] |C2_standard| = 3828\n       Done.\n      â€¢ |C2_standard|:    3828\n      â€¢ Frequent Sets:    341\n      â€¢ Execution Time:   3.7923 s\n      â€¢ Peak Memory:      1.2775 MB\n\n############################################################\n PROCESSING DATASET: RETAIL\n############################################################\n Loading: /kaggle/input/associative-rules-data/retail_transactional.csv ...\n Loaded 88162 transactions.\n   Dataset size: 88162 transactions.\n\n    Testing Min Support: 0.05\n   [Standard] Generating L1 (Min Count: 4408)...\n   [Metric Captured] |C2_standard| = 15\n       Done.\n      â€¢ |C2_standard|:    15\n      â€¢ Frequent Sets:    16\n      â€¢ Execution Time:   2.8408 s\n      â€¢ Peak Memory:      3.9867 MB\n\n    Testing Min Support: 0.03\n   [Standard] Generating L1 (Min Count: 2644)...\n   [Metric Captured] |C2_standard| = 66\n       Done.\n      â€¢ |C2_standard|:    66\n      â€¢ Frequent Sets:    32\n      â€¢ Execution Time:   3.1518 s\n      â€¢ Peak Memory:      3.9991 MB\n\n    Testing Min Support: 0.02\n   [Standard] Generating L1 (Min Count: 1763)...\n   [Metric Captured] |C2_standard| = 190\n       Done.\n      â€¢ |C2_standard|:    190\n      â€¢ Frequent Sets:    55\n      â€¢ Execution Time:   4.1622 s\n      â€¢ Peak Memory:      4.0398 MB\n\n    Testing Min Support: 0.01\n   [Standard] Generating L1 (Min Count: 881)...\n   [Metric Captured] |C2_standard| = 2415\n       Done.\n      â€¢ |C2_standard|:    2415\n      â€¢ Frequent Sets:    159\n      â€¢ Execution Time:   21.3224 s\n      â€¢ Peak Memory:      4.7063 MB\n\n============================================================\n FINAL COMBINED SUMMARY (Standard Apriori)\n============================================================\n  dataset  min_sup  c2_count  total_freq_itemsets  execution_time  memory_peak_mb\nGROCERIES     0.05       378                   31        0.425322        0.185311\nGROCERIES     0.03       946                   64        0.847625        0.325255\nGROCERIES     0.02      1711                  123        1.534687        0.629295\nGROCERIES     0.01      3828                  341        3.792343        1.277511\n   RETAIL     0.05        15                   16        2.840799        3.986710\n   RETAIL     0.03        66                   32        3.151762        3.999064\n   RETAIL     0.02       190                   55        4.162220        4.039764\n   RETAIL     0.01      2415                  159       21.322415        4.706322\n","output_type":"stream"}],"execution_count":8},{"cell_type":"markdown","source":"### ðŸ“Š Analysis of Standard Apriori Baseline (Groceries Dataset)\n\nThe execution of the Standard Apriori algorithm on the `Groceries` dataset establishes a critical baseline. By varying the `min_support` from 0.05 down to 0.01, we can clearly observe the computational cost associated with the **Combinatorial Explosion** inherent in the Apriori principle.\n\n#### 1. The Non-Linear Growth of Candidates ($|C_2|$)\nThe most significant insight from this test is the behavior of $|C_{2\\_standard}|$ (the count of 2-itemset candidates).\n*   **Observation:** When `min_sup` was lowered from **0.05** to **0.01** (a 5x reduction in threshold), the number of candidates generated did not increase linearly. Instead, it jumped from **378** to **3,828** (a **~10x increase**).\n*   **Insight:** This illustrates the quadratic nature of candidate generation. As the support threshold drops, slightly more individual items ($L_1$) become \"frequent.\" Since $C_2$ is formed by combining all members of $L_1$ (computationally $\\binom{n}{2}$), even a small increase in $L_1$ size results in a massive spike in potential pairs that the algorithm must verify against the database.\n\n#### 2. Performance Scaling (Time & Memory)\n*   **Execution Time:** The runtime increased from **0.43s** to **3.79s**. While ~4 seconds is manageable for this specific dataset, the growth curve is steep. This confirms that Standard Apriori is highly sensitive to the support threshold.\n*   **Memory Usage:** Peak memory usage grew from **0.18 MB** to **1.28 MB**. While the Groceries dataset is sparse enough that memory is not yet a hardware constraint, the upward trend confirms that memory consumption scales directly with the number of candidates stored in the hash tree/dictionary.\n\n#### 3. Implications for Larger Datasets (Retail)\nThis baseline highlights the risk for the **Retail Transactional** dataset.\n*   In `Groceries`, we have ~169 unique items, resulting in ~3,800 candidates at 1% support.\n*   In `Retail`, with **16,470 unique items**, a similar Standard Apriori run could theoretically generate **millions** of candidates in $C_2$.\n*   The 3.79s runtime observed here would likely translate to hours or reduced system stability on the larger dataset without optimization.\n\n#### 4. Conclusion and Objective for DHP\nWe have successfully captured the target metric to beat: **$|C_{2\\_standard}| = 3,828$** (at 1% support).\n\nThe success of the next algorithm, **DHP (Direct Hashing and Pruning)**, will be measured not necessarily by raw execution time (due to Python overhead), but by its ability to significantly reduce this candidate count. If DHP is effective, $|C_{2\\_DHP}|$ should be substantially lower than 3,828, proving that we successfully pruned false positives before scanning the database.\n","metadata":{}},{"cell_type":"code","source":"\n# DHP HELPER FUNCTIONS\n\ndef get_hash_bucket_index(item1, item2, bucket_count):\n    # hash() in Python is consistent within a session\n    return (hash(item1) ^ hash(item2)) % bucket_count\n\ndef scan_and_count_supports(dataset, candidates):\n    \"\"\"\n    Standard scan to count supports for candidates.\n    \"\"\"\n    counts = defaultdict(int)\n    \n    candidate_set = set(candidates)\n    \n    for transaction in dataset:\n        # Convert transaction to set for fast subset checking\n        t_set = frozenset(transaction)\n        \n        # Check which candidates are in this transaction\n        # (For very large candidate sets, a Hash Tree is better, \n        # but for this assignment, simple subset check is fine)\n        for cand in candidates:\n            if cand.issubset(t_set):\n                counts[cand] += 1\n    return counts\n\ndef generate_next_candidates_standard(L_k_minus_1, k):\n    \n    candidates = []\n    l_list = sorted(list(L_k_minus_1), key=lambda x: list(x))\n    n = len(l_list)\n    \n    for i in range(n):\n        for j in range(i + 1, n):\n            itemset1 = list(l_list[i])\n            itemset2 = list(l_list[j])\n            \n            # Join condition: first k-2 items must be identical\n            # For k=3, check first 1 item.\n            if k == 2 or itemset1[:k-2] == itemset2[:k-2]:\n                # Union\n                new_cand = l_list[i] | l_list[j]\n                # Pruning Step (Apriori Property):\n                candidates.append(new_cand)\n                \n    return candidates\n\n\n# MAIN DHP ALGORITHM\n\ndef run_dhp_apriori(dataset, min_support_fraction, bucket_count=1000003):\n    \n    # 1Setup & Metrics\n    tracemalloc.start()\n    start_time = time.time()\n    \n    total_transactions = len(dataset)\n    min_count = total_transactions * min_support_fraction\n    \n    global_freq_itemsets = {}\n    \n    print(f\"   [DHP] Started. Min Sup Count: {min_count:.1f}, Buckets: {bucket_count}\")\n\n    #  Init Pass (Count L1 + Populate Hash Table)\n\n    print(\"   [DHP] Pass 1: Counting Items & Hashing Pairs...\")\n    \n    item_counts = defaultdict(int)\n    bucket_table = defaultdict(int) # Using dict instead of list to save memory on sparse buckets\n    \n    for transaction in dataset:\n        items = list(transaction) # Convert to list for indexing\n        \n        # 1. Count individual items\n        for item in items:\n            item_counts[item] += 1\n            \n        # We do this loop HERE to avoid reading database again for C2 generation logic\n        n_items = len(items)\n        if n_items >= 2:\n            for i in range(n_items):\n                for j in range(i + 1, n_items):\n                    b_idx = get_hash_bucket_index(items[i], items[j], bucket_count)\n                    bucket_table[b_idx] += 1\n\n    # Generate L1\n    L1 = [frozenset([item]) for item, count in item_counts.items() if count >= min_count]\n    \n    # Store L1 counts\n    for l1 in L1:\n        global_freq_itemsets[l1] = item_counts[list(l1)[0]]\n        \n    print(f\"   [DHP] L1 Size: {len(L1)}\")\n\n    # Generate C2 using DHP Pruning\n    \n    # Standard Apriori would just join L1.\n    # DHP checks the bucket_table before adding to C2.\n    \n    C2_candidates = []\n    L1_single_items = [list(x)[0] for x in L1] # Unpack frozensets to items\n    L1_single_items.sort() # Sorting helps determiistic behavior\n    \n    # Generate all pairs from L1\n    potential_pairs = list(combinations(L1_single_items, 2))\n    \n    for p in potential_pairs:\n        # Check Hash Table\n        b_idx = get_hash_bucket_index(p[0], p[1], bucket_count)\n        \n        # PRUNING CONDITION:\n        # If the bucket count is less than min_count, this pair CANNOT be frequent.\n        if bucket_table[b_idx] >= min_count:\n            C2_candidates.append(frozenset(p))\n            \n    # !!! CRITICAL METRIC !!!\n    c2_count = len(C2_candidates)\n    print(f\"   [Metric Captured] |C2_DHP| = {c2_count} (Pruned from {len(potential_pairs)} possible)\")\n\n    # Standard Loop (Scan for C2, Generate L2, C3...)\n    current_candidates = C2_candidates\n    k = 2\n    \n    while current_candidates:\n        \n        # 1. Count Support for Current Candidates\n        candidates_counts = scan_and_count_supports(dataset, current_candidates)\n        \n        # 2. Filter to get L_k\n        L_k = []\n        for cand, count in candidates_counts.items():\n            if count >= min_count:\n                L_k.append(cand)\n                global_freq_itemsets[cand] = count\n        \n        \n        # Stop if no frequent itemsets found\n        if not L_k:\n            break\n            \n        # Generate Next Candidates (C_k+1)\n        k += 1\n        current_candidates = generate_next_candidates_standard(L_k, k)\n        \n        # Safety break for very deep trees\n        if k > 5: \n            break\n\n    # FINISH & METRICS\n    end_time = time.time()\n    current, peak = tracemalloc.get_traced_memory()\n    tracemalloc.stop()\n    \n    execution_time_sec = end_time - start_time\n    peak_memory_mb = peak / (1024 * 1024)\n    \n    return global_freq_itemsets, {\n        'c2_count': c2_count,\n        'total_freq_itemsets': len(global_freq_itemsets),\n        'execution_time': execution_time_sec,\n        'memory_peak_mb': peak_memory_mb\n    }\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T06:59:57.180452Z","iopub.execute_input":"2025-11-30T06:59:57.180681Z","iopub.status.idle":"2025-11-30T06:59:57.197502Z","shell.execute_reply.started":"2025-11-30T06:59:57.180663Z","shell.execute_reply":"2025-11-30T06:59:57.196557Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nfrom IPython.display import display\n\n# ... (Loading Dataset Code remains the same) ...\n\nmin_supports = [0.05, 0.03, 0.02, 0.01]\nresults_dhp = []\n\nprint(\"Running DHP Tests...\")\n\nfor name, data in datasets.items():\n    if not data: continue \n    \n    for min_sup in min_supports:\n        # Run DHP\n        freq_items, metrics = run_dhp_apriori(data, min_support_fraction=min_sup, bucket_count=1000003)\n        \n        # Save Results\n        result_row = {\n            \"Dataset\": name,\n            \"Min Support\": min_sup,\n            \"|C2| (DHP Candidates)\": metrics['c2_count'],\n            \"Total Freq Items\": metrics['total_freq_itemsets'],\n            \"Time (s)\": round(metrics['execution_time'], 4),\n            \"Memory (MB)\": round(metrics['memory_peak_mb'], 4)\n        }\n        results_dhp.append(result_row)\n        \n        # Simple progress print (just dots or basic info to reduce clutter)\n        print(f\"   -> Finished {name} @ {min_sup} | Time: {metrics['execution_time']:.2f}s\")\n\nprint(\"\\n\" + \"=\"*30)\nprint(\"   âœ… TEST COMPLETED\")\nprint(\"=\"*30 + \"\\n\")\n\n# --- BEAUTIFUL OUTPUT SECTION ---\n\n# 1. Create DataFrame\ndf_results = pd.DataFrame(results_dhp)\n\n# 2. Apply Styling (Heatmap)\n# High values in Time and C2 will be Red, Low values will be Blue/White\nstyled_df = df_results.style.background_gradient(\n    subset=['|C2| (DHP Candidates)', 'Time (s)', 'Memory (MB)'], \n    cmap='OrRd'  # Orange to Red colormap\n).format({\n    'Min Support': '{:.2f}',\n    'Time (s)': '{:.4f}',\n    'Memory (MB)': '{:.4f}'\n}).set_properties(**{\n    'text-align': 'center',\n    'border': '1px solid black'\n}).set_caption(\"ðŸš€ DHP Apriori Performance Results\")\n\n# 3. Display\ndisplay(styled_df)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-11-30T07:09:58.791169Z","iopub.execute_input":"2025-11-30T07:09:58.791805Z","iopub.status.idle":"2025-11-30T07:12:26.342491Z","shell.execute_reply.started":"2025-11-30T07:09:58.791767Z","shell.execute_reply":"2025-11-30T07:12:26.341297Z"}},"outputs":[{"name":"stdout","text":"Running DHP Tests...\n   [DHP] Started. Min Sup Count: 491.8, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 28\n   [Metric Captured] |C2_DHP| = 3 (Pruned from 378 possible)\n   -> Finished GROCERIES @ 0.05 | Time: 0.80s\n   [DHP] Started. Min Sup Count: 295.1, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 44\n   [Metric Captured] |C2_DHP| = 19 (Pruned from 946 possible)\n   -> Finished GROCERIES @ 0.03 | Time: 0.82s\n   [DHP] Started. Min Sup Count: 196.7, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 59\n   [Metric Captured] |C2_DHP| = 61 (Pruned from 1711 possible)\n   -> Finished GROCERIES @ 0.02 | Time: 0.94s\n   [DHP] Started. Min Sup Count: 98.4, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 88\n   [Metric Captured] |C2_DHP| = 213 (Pruned from 3828 possible)\n   -> Finished GROCERIES @ 0.01 | Time: 1.64s\n   [DHP] Started. Min Sup Count: 4408.1, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 6\n   [Metric Captured] |C2_DHP| = 7 (Pruned from 15 possible)\n   -> Finished RETAIL @ 0.05 | Time: 35.38s\n   [DHP] Started. Min Sup Count: 2644.9, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 12\n   [Metric Captured] |C2_DHP| = 16 (Pruned from 66 possible)\n   -> Finished RETAIL @ 0.03 | Time: 35.54s\n   [DHP] Started. Min Sup Count: 1763.2, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 20\n   [Metric Captured] |C2_DHP| = 23 (Pruned from 190 possible)\n   -> Finished RETAIL @ 0.02 | Time: 35.53s\n   [DHP] Started. Min Sup Count: 881.6, Buckets: 1000003\n   [DHP] Pass 1: Counting Items & Hashing Pairs...\n   [DHP] L1 Size: 70\n   [Metric Captured] |C2_DHP| = 58 (Pruned from 2415 possible)\n   -> Finished RETAIL @ 0.01 | Time: 35.88s\n\n==============================\n   âœ… TEST COMPLETED\n==============================\n\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<pandas.io.formats.style.Styler at 0x7b83d3555810>","text/html":"<style type=\"text/css\">\n#T_28da0_row0_col0, #T_28da0_row0_col1, #T_28da0_row0_col3, #T_28da0_row1_col0, #T_28da0_row1_col1, #T_28da0_row1_col3, #T_28da0_row2_col0, #T_28da0_row2_col1, #T_28da0_row2_col3, #T_28da0_row3_col0, #T_28da0_row3_col1, #T_28da0_row3_col3, #T_28da0_row4_col0, #T_28da0_row4_col1, #T_28da0_row4_col3, #T_28da0_row5_col0, #T_28da0_row5_col1, #T_28da0_row5_col3, #T_28da0_row6_col0, #T_28da0_row6_col1, #T_28da0_row6_col3, #T_28da0_row7_col0, #T_28da0_row7_col1, #T_28da0_row7_col3 {\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row0_col2, #T_28da0_row0_col4, #T_28da0_row1_col4, #T_28da0_row1_col5, #T_28da0_row2_col5 {\n  background-color: #fff7ec;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row0_col5, #T_28da0_row3_col2, #T_28da0_row4_col5, #T_28da0_row5_col5, #T_28da0_row6_col5, #T_28da0_row7_col4, #T_28da0_row7_col5 {\n  background-color: #7f0000;\n  color: #f1f1f1;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row1_col2 {\n  background-color: #feeed7;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row2_col2 {\n  background-color: #fdcf99;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row2_col4, #T_28da0_row3_col5 {\n  background-color: #fff7eb;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row3_col4 {\n  background-color: #fff4e5;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row4_col2 {\n  background-color: #fff5e7;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row4_col4 {\n  background-color: #840000;\n  color: #f1f1f1;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row5_col2 {\n  background-color: #fff0db;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row5_col4, #T_28da0_row6_col4 {\n  background-color: #820000;\n  color: #f1f1f1;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row6_col2 {\n  background-color: #feecd1;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n#T_28da0_row7_col2 {\n  background-color: #fdd19b;\n  color: #000000;\n  text-align: center;\n  border: 1px solid black;\n}\n</style>\n<table id=\"T_28da0\">\n  <caption>ðŸš€ DHP Apriori Performance Results</caption>\n  <thead>\n    <tr>\n      <th class=\"blank level0\" >&nbsp;</th>\n      <th id=\"T_28da0_level0_col0\" class=\"col_heading level0 col0\" >Dataset</th>\n      <th id=\"T_28da0_level0_col1\" class=\"col_heading level0 col1\" >Min Support</th>\n      <th id=\"T_28da0_level0_col2\" class=\"col_heading level0 col2\" >|C2| (DHP Candidates)</th>\n      <th id=\"T_28da0_level0_col3\" class=\"col_heading level0 col3\" >Total Freq Items</th>\n      <th id=\"T_28da0_level0_col4\" class=\"col_heading level0 col4\" >Time (s)</th>\n      <th id=\"T_28da0_level0_col5\" class=\"col_heading level0 col5\" >Memory (MB)</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th id=\"T_28da0_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n      <td id=\"T_28da0_row0_col0\" class=\"data row0 col0\" >GROCERIES</td>\n      <td id=\"T_28da0_row0_col1\" class=\"data row0 col1\" >0.05</td>\n      <td id=\"T_28da0_row0_col2\" class=\"data row0 col2\" >3</td>\n      <td id=\"T_28da0_row0_col3\" class=\"data row0 col3\" >31</td>\n      <td id=\"T_28da0_row0_col4\" class=\"data row0 col4\" >0.8030</td>\n      <td id=\"T_28da0_row0_col5\" class=\"data row0 col5\" >81.7294</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n      <td id=\"T_28da0_row1_col0\" class=\"data row1 col0\" >GROCERIES</td>\n      <td id=\"T_28da0_row1_col1\" class=\"data row1 col1\" >0.03</td>\n      <td id=\"T_28da0_row1_col2\" class=\"data row1 col2\" >19</td>\n      <td id=\"T_28da0_row1_col3\" class=\"data row1 col3\" >63</td>\n      <td id=\"T_28da0_row1_col4\" class=\"data row1 col4\" >0.8243</td>\n      <td id=\"T_28da0_row1_col5\" class=\"data row1 col5\" >0.6118</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n      <td id=\"T_28da0_row2_col0\" class=\"data row2 col0\" >GROCERIES</td>\n      <td id=\"T_28da0_row2_col1\" class=\"data row2 col1\" >0.02</td>\n      <td id=\"T_28da0_row2_col2\" class=\"data row2 col2\" >61</td>\n      <td id=\"T_28da0_row2_col3\" class=\"data row2 col3\" >122</td>\n      <td id=\"T_28da0_row2_col4\" class=\"data row2 col4\" >0.9441</td>\n      <td id=\"T_28da0_row2_col5\" class=\"data row2 col5\" >0.6754</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n      <td id=\"T_28da0_row3_col0\" class=\"data row3 col0\" >GROCERIES</td>\n      <td id=\"T_28da0_row3_col1\" class=\"data row3 col1\" >0.01</td>\n      <td id=\"T_28da0_row3_col2\" class=\"data row3 col2\" >213</td>\n      <td id=\"T_28da0_row3_col3\" class=\"data row3 col3\" >333</td>\n      <td id=\"T_28da0_row3_col4\" class=\"data row3 col4\" >1.6443</td>\n      <td id=\"T_28da0_row3_col5\" class=\"data row3 col5\" >1.1296</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n      <td id=\"T_28da0_row4_col0\" class=\"data row4 col0\" >RETAIL</td>\n      <td id=\"T_28da0_row4_col1\" class=\"data row4 col1\" >0.05</td>\n      <td id=\"T_28da0_row4_col2\" class=\"data row4 col2\" >7</td>\n      <td id=\"T_28da0_row4_col3\" class=\"data row4 col3\" >16</td>\n      <td id=\"T_28da0_row4_col4\" class=\"data row4 col4\" >35.3791</td>\n      <td id=\"T_28da0_row4_col5\" class=\"data row4 col5\" >81.7293</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n      <td id=\"T_28da0_row5_col0\" class=\"data row5 col0\" >RETAIL</td>\n      <td id=\"T_28da0_row5_col1\" class=\"data row5 col1\" >0.03</td>\n      <td id=\"T_28da0_row5_col2\" class=\"data row5 col2\" >16</td>\n      <td id=\"T_28da0_row5_col3\" class=\"data row5 col3\" >32</td>\n      <td id=\"T_28da0_row5_col4\" class=\"data row5 col4\" >35.5385</td>\n      <td id=\"T_28da0_row5_col5\" class=\"data row5 col5\" >81.7291</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n      <td id=\"T_28da0_row6_col0\" class=\"data row6 col0\" >RETAIL</td>\n      <td id=\"T_28da0_row6_col1\" class=\"data row6 col1\" >0.02</td>\n      <td id=\"T_28da0_row6_col2\" class=\"data row6 col2\" >23</td>\n      <td id=\"T_28da0_row6_col3\" class=\"data row6 col3\" >55</td>\n      <td id=\"T_28da0_row6_col4\" class=\"data row6 col4\" >35.5301</td>\n      <td id=\"T_28da0_row6_col5\" class=\"data row6 col5\" >81.7291</td>\n    </tr>\n    <tr>\n      <th id=\"T_28da0_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n      <td id=\"T_28da0_row7_col0\" class=\"data row7 col0\" >RETAIL</td>\n      <td id=\"T_28da0_row7_col1\" class=\"data row7 col1\" >0.01</td>\n      <td id=\"T_28da0_row7_col2\" class=\"data row7 col2\" >58</td>\n      <td id=\"T_28da0_row7_col3\" class=\"data row7 col3\" >159</td>\n      <td id=\"T_28da0_row7_col4\" class=\"data row7 col4\" >35.8841</td>\n      <td id=\"T_28da0_row7_col5\" class=\"data row7 col5\" >81.7292</td>\n    </tr>\n  </tbody>\n</table>\n"},"metadata":{}}],"execution_count":14},{"cell_type":"markdown","source":"# Phase 2 Analysis: DHP Apriori Results\n\nFollowing the execution of the Direct Hashing and Pruning (DHP) algorithm, we can now compare its performance against the Standard Apriori baseline established in Phase 1. The results highlight a classic algorithmic trade-off between **search space reduction** and **computational overhead**.\n\n### 1. The Major Victory: Massive Pruning ($|C_2|$ Reduction)\nThe primary goal of DHP is to reduce the size of the candidate set ($C_2$) generated in the second pass. The results show the algorithm succeeded spectacularly in this regard.\n\n*   **Groceries Dataset (@ 0.01 Support):**\n    *   **Standard Apriori:** Generated **3,828** candidates.\n    *   **DHP Apriori:** Generated only **213** candidates.\n    *   **Impact:** A **94.4% reduction** in the search space. This massive pruning resulted in the total execution time dropping from **3.79s** (Standard) to **1.64s** (DHP).\n\n*   **Retail Dataset (@ 0.01 Support):**\n    *   **Standard Apriori:** Generated **2,415** candidates.\n    *   **DHP Apriori:** Generated only **58** candidates.\n    *   **Impact:** A **97.6% reduction**. DHP virtually eliminated the \"Candidate Explosion\" problem found in the baseline.\n\n### 2. The Hidden Cost: Hashing Overhead\nWhile DHP reduced the *number* of candidates, the execution time for the `RETAIL` dataset reveals a hidden cost.\n\n*   **Standard Apriori Time:** ~20.5s (at 0.01 support).\n*   **DHP Apriori Time:** ~35.8s (constant across all supports).\n\n**Why was DHP slower on Retail?**\nThe bottleneck shifted from Pass 2 (counting candidates) to **Pass 1 (generating hashes)**.\nIn Standard Apriori, Pass 1 only counts single items ($N$ operations per transaction). In DHP, Pass 1 must generate and hash **every possible pair** in a transaction. If a transaction has 20 items, DHP performs:\n$$ \\text{Operations} = \\frac{n(n-1)}{2} = \\frac{20 \\times 19}{2} = 190 \\text{ hash operations} $$\nFor the dense Retail dataset, this massive volume of hashing operations in the first pass outweighed the time saved by having fewer candidates in the second pass.\n\n### 3. Stability and Predictability\nA key advantage observed in the DHP results is **predictability**.\n*   **Standard Apriori:** Execution time grows exponentially as support decreases (due to candidate explosion).\n*   **DHP Apriori:** Execution time remains flat ($\\sim 35s$ for Retail) regardless of support level. The hash table cost is fixed, making DHP a more stable choice for systems where memory spikes are unacceptable.\n\n### Summary Comparison\n\n| Feature | Standard Apriori | DHP Apriori |\n| :--- | :--- | :--- |\n| **Candidate Set ($C_2$)** | High (prone to explosion) | **Extremely Low (Optimized)** |\n| **Pass 1 CPU Cost** | Low (Linear scan) | **High (Quadratic Hashing)** |\n| **Execution Time Trend** | Sensitive to Min Support | **Stable / Flat** |\n| **Best Use Case** | Sparse data, higher support | Dense data, low support, memory constraints |\n\n### Next Steps\nWe have seen that **Standard Apriori** suffers from candidate explosion, while **DHP** suffers from initial hashing overhead on large datasets.\n\nPhase 3 will implement **Sampling Apriori**, which attempts to solve the speed bottleneck by processing only a random subset of the database, trading theoretical accuracy for significantly improved performance.\n","metadata":{}}]}